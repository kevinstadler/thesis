\section{A Markov chain model of neutral evolution}
\index{Bayesian inference}\index{Iterated Learning}
\label{sec:realigriffiths}

One formal model of neutral evolution (i.e.~copying of linguistic traits in the absence of any replicator or interactor selection) that makes particular reference to the temporal dynamics of changes is Reali \& Griffiths model of regularisation by Bayesian learners~(\citeyear{Reali2009,Reali2010}).

%\subsection{Model description}
At its core, \citeauthor{Reali2009} present a model of frequency learning by Bayesian inference. In their particular framing, an individual is trying to infer the relative frequencies $\theta_i \in [0,1]$ of different variants $i=1\ldots n$ based on some input data as well as prior beliefs about what the true values of $\theta_i$ are likely to be. These prior beliefs act as \emph{inductive biases} and are captured by the \emph{prior}, represented by a probability distribution $f(\vec{\theta})$ defined over all possible values of $\vec{\theta}$.

For the simple case of two competing variants, even though the individual is technically inferring two complementary relative frequencies $\theta_1, \theta_2$, we can limit our analysis to the problem of inferring $\theta_1$, since trivially $\theta_2=1-\theta_1$. The model can easily be extended from the \emph{binomial}~(two-variant) outcome to \emph{multinomial} outcomes, i.e. with three or more competing variants but, without loss of generality, we will limit our demonstration to the case of two competing variants. To simplify notation we will henceforth also write $\theta$ to refer to $\theta_1$.

While any continuous probability distribution over the interval $[0,1]$ could serve as a prior, the authors choose the \emph{Beta distribution}, whose probability density function is defined as
\begin{equation}
f(x;\alpha,\beta) = \frac{1}{B(\alpha,\beta)} x^{\alpha-1}(1-x)^{\beta-1}\;,
\end{equation}
where $B(.)$ is the \emph{Beta function}.

Because we are interested in a \emph{neutral} model that is not a priori biased in favour or against either of the competing variants, the shape of the prior distribution over the support will have to be \emph{symmetric}: the prior probability density of $\theta$ taking a certain value, $f(\theta)$, should be the same as its probability of taking the complementary value $f(1-\theta)$. This can be achieved by setting the Beta distribution's shape parameters $\alpha,\beta$ to the same value. Consequently the authors use prior distributions of the form
\begin{equation}
\Theta \sim Beta(\frac{\alpha}{2},\frac{\alpha}{2})\;.
\end{equation}
with just a single parameter, $\alpha$, which controls the degree of \emph{regularisation}. Figure~\ref{fig:priors} shows the effect of this parameter on the prior distribution. For a value of $\frac{\alpha}{2}=1$ the prior distribution is uniform: not only is the individual not biased towards any of the variants (the distribution is symmetric), their estimate of the underlying frequency~$\theta$ is not biased towards any particular frequency region in~$[0,1]$ either. The same isn't true when $\frac{\alpha}{2}\ne 1$: for values~$<1$, the inference of $\theta$ is explicitly geared towards more extreme relative frequencies closer to $0$ or $100\%$ usage -- the model implements a \emph{regularisation bias}. The opposite is the case when~$>1$ which \emph{a priori} favours values of $\theta$ around the $0.5$ mark. Agents employing such a setting are inclined to infer more mixed usage of the competing variants than suggested by their learning data alone.

% In the case of $K\ge3$ competing variants the Beta distribution prior with shape parameters $\frac{\alpha}{2}$ is simply replaced by a Dirichlet prior with parameters $\frac{\alpha}{K}$.


\begin{figure}[htbp]

{\centering \subfloat[Prior distributions for three different levels of~$\alpha$.\label{fig:priors1}]{\includegraphics[width=\maxwidth]{figure/priors-1} }
\subfloat[Posterior distributions after observing $N=10$ data points with various input distributions ($x=5,2,0$), regularisation parameters as in (a).\label{fig:priors2}]{\includegraphics[width=\maxwidth]{figure/priors-2} }

}

\caption[Examples of Beta distribution priors and posteriors with three different levels of the regularisation parameter~]{Examples of Beta distribution priors and posteriors with three different levels of the regularisation parameter~$\alpha/2$.}\label{fig:priors}
\end{figure}



The particular choice of prior distribution~(Beta or Dirichlet for the multinomial case) has elegant mathematical properties: when a learner receives an input sample of size~$N$, where $0\le x \le N$ of the tokens were instances of the variant whose frequency $\theta$ they are trying to infer, then the posterior is again a Beta distribution, namely
\begin{equation}
\label{eq:posterior}
\Theta|x \sim Beta(x+\frac{\alpha}{2}, N-x+\frac{\alpha}{2})\;. % TODO fix notation
\end{equation}

Following this inference step, there is still the question of how the posterior distribution is translated into actual production behaviour, which provides us with testable predictions of the model. % after all, we need production behaviour to make a full iterated learning model.
Here, we will consider three different ways for an individual to generate their own productions~$x'$ based on the learning sample~$x$ that they themselves received. % deriving a specific value~$\theta$ from the posterior distribution,
The first two were also treated by \citet{Reali2009}, the third covered by \citet[p.156]{Ferdinand2015}:

\begin{description}
\item[Sampling from the posterior:] when generating new productions directly from the posterior, the probability that a \emph{sampling learner} produces a particular variant $x'$~times out of a total of $N$~productions is distributed according to a \emph{Betabinomial distribution} with the same parameters as the posterior distribution in Equation~\ref{eq:posterior}, i.e.
\begin{equation}\label{eq:usmsampling}
X'|x \sim BB(x+\frac{\alpha}{2}, N-x+\frac{\alpha}{2}, N) \;.
\end{equation}

\item[Adopting the \emph{average} of the posterior:] instead of sampling from the posterior for every production, an individual could deterministically select the mean of the posterior distribution, which is
\begin{equation}\label{eq:usmaveraging}
\hat{\theta}=\frac{x+\frac{\alpha}{2}}{N+\alpha}\;.%  close to the observed relative frequency $\frac{x_1}{N}$.
\end{equation}
The productions of a Bayesian learner who deterministally chooses the parameter $\hat{\theta}$ are then distributed according to a Binomial %(or, in the case of multinomial outcomes, Multinomial)
distribution,
\begin{equation}\label{eq:usmmap}
X'|x \sim Bin(N, \hat{\theta}) \;.
\end{equation}
While \citet{Reali2009} call this a `MAP' learner, we will refer to this mechanism of selecting a hypothesis as the \emph{averager} strategy.

\item[Adopting the \emph{mode} of the posterior (\emph{maximum a posteriori}):] The posterior distribution's mode, where the probability density function is highest, can be found at
\begin{equation}
\theta_{MAP} = \arg\max_{\theta} f(\theta|x) = \frac{x+\frac{\alpha}{2}-1}{N+\alpha-2}\;,
\end{equation}
except when~$x=0$ or~$x=N$, in which case the resulting posterior Beta distribution is \emph{j-shaped}, with the mode falling on~$0$ or~$1$, respectively. When such a \emph{MAP learner} has adopted the mode as their production probability then their own productions are distributed according to a Binomial distribution with $p=\theta_{MAP}$, i.e.
\begin{equation}
X'|x \sim Bin(N, \theta_{MAP}) \;.
\end{equation}
\end{description}

One way in which the impact of these different ways of sampling data (either directly from the posterior or by first deterministically selecting a $\theta$) can be exemplified is by visualising the \emph{average production} of a learner who is inferring the underlying distribution based on the input sample they just observed. This data is shown in Figure~\ref{fig:meanmapping}, which maps the different possible input distributions~(along the x-axis) to the average output productions~$\pm$~their standard deviation. The identity function $x=y$, equivalent to pure probability matching, is shown for reference. In this graphical representation, a mapping function that leads to increased \emph{regularisation} should map input proportions between $0$~and~$50\%$ to even \emph{lower} output proportions, while input proportions~$>50\%$ should yield output proportions even closer to~$100\%$.

What is evident from Figure~\ref{fig:meanmapping} is that the only method which on average leads to regularisation at every iteration is the \emph{maximum a posteriori} method with $\alpha\le 1$. None of the other mapping functions are consistently regularising. Rather, as was pointed out by~\citet[p.176]{Ferdinand2015} both data production methods discussed by \citeauthor{Reali2009} rely on mechanisms that merely increase the sample variability \emph{in either direction}, until the system drifts into a state of categorical presence of one variant only. This contrasts with the regularising mapping functions of the Utterance Selection Model shown in Section~\ref{sec:usmregularisation}, which systematically increase the proportion of whichever variant is currently more prevalent. We will return to a critique of the present regularisation model in the next section. % \citep{Spike??}
% TODO point out the 1/10 -> 2/10 transition probability

\begin{figure}[htbp]

{\centering \subfloat[Input/mean output mapping when sampling from the posterior.\label{fig:meanmapping1}]{\includegraphics[width=\maxwidth]{figure/meanmapping-1} }
\subfloat[Input/mean output mapping when selecting the \emph{average} of the posterior as the hypothesis. As pointed out by \citet{Ferdinand2015}, the \emph{mean} output of this model is identical to that of the sampler shown above, only that the averager exhibits different amounts of sampling error about this mean, depending on the input frequency.\label{fig:meanmapping2}]{\includegraphics[width=\maxwidth]{figure/meanmapping-2} }
\subfloat[Input/mean output mapping when selecting the \emph{maximum} of the posterior as the hypothesis~(MAP). With~$\frac{\alpha}{2}=1$~(middle panel) this strategy is identical to pure frequency matching, while MAP with $\frac{\alpha}{2}<1$~(left panel) is the only strategy that, \emph{on average}, leads to regularisation in one iteration.\label{fig:meanmapping3}]{\includegraphics[width=\maxwidth]{figure/meanmapping-3} }

}

\caption[Input to mean-output mapping for the three ways of producing data from the posterior and three levels of the regularisation parameter]{Input to mean-output mapping for the three ways of producing data from the posterior and three levels of the regularisation parameter. The three settings of $\alpha$ capture inductive biases ranging from regularisation~($\frac{\alpha}{2}=.25$, left column) to de-regularisation ($\frac{\alpha}{2}=5$, right column).}\label{fig:meanmapping}
\end{figure}



% With these three different models up our sleeve, we can turn to actual productions.

\subsection{Representing Bayesian Iterated Learning as a Markov chain}\index{Markov model}
\label{sec:markovmodel}

While the model presented above captures frequency learning by Bayesian inference within one individual, it is interesting to ask how the productions of a sequence of such learners would develop over time when one individual's output serves as the learning input of another. To do this, we can analyse the interactions between repeated learning input and production output as a \emph{Markov chain}, a simple modelling tool for understanding systems which can be in one of a finite number of states that they switch between probabilistically.

More formally, a Markov model can be defined by specifying conditional transition probabilities $P(X_{t+1}=x'|X_t=x)$ between a number of discrete states $x,x'\in\mathcal{S}$, which we call the Markov model's \emph{state space}. The Markov model is completely described by a function $P: \mathcal{S}\times\mathcal{S} \rightarrow [0,1]$ where the transition probabilities \emph{out} of any given state have to sum to one, i.e.
\begin{equation}
\sum_{x'\in\mathcal{S}} P(X_{t+1}=x'|X_t=x) = 1 \qquad\forall\; x \in \mathcal{S}\;.
\end{equation}

In the case of the Bayesian inference model above, there are two equally valid ways in which it could be translated into a Markov model, based on how the state space $\mathcal{S}$ is construed. The logical alternation between learning parameter $\theta$ and production of $x$ tokens of a specific variant out of $N$~total productions allows for both a characterisation of the Markov model as transitioning from one individual's posterior distribution $f(\theta|x)$ to another or, alternatively, from one individual's number of productions~$x$ to the next.

%The state space of the respective Markov models would be either defined by the set of all possible posterior distributions, or alternatively by the set of all possible productions. In the former case the respective transition probabilities would then capture the probability of an individual's posterior distribution resulting in the following individual having a particular posterior distribution or, in the case of the production-centric view, simply the probability of one individual producing a certain number of tokens of variant~1 given how many tokens of that type the previous learner produced.
%For sake of simplicity and increased interpretability, 

To define the state space, we have to set a fixed size of productions~$N$, from which a new learner has to infer the underlying production frequency~$\theta$.

An example of such a transition matrix for $N=10, \alpha=0.5$ is found in Table~\ref{tbl:transitionmatrixsample}. This particular matrix is created based on the assumption that learners sample their data directly from the posterior distribution they computed from the input they received.

Compare this to Table~\ref{tbl:transitionmatrixmap}, which is based on a chain of learners that deterministically select the mode $\theta_{MAP}$ of the posterior distribution $f(\theta|x)$ as their estimate of $\theta$. Their data production probabilities are consequently distributed according to a Binomial distribution with $p=\theta_{MAP}$, so the rows of this transition matrix are equivalent to this Binomial distribution.

% latex table generated in R 3.2.3 by xtable 1.8-2 package
% Thu Aug 25 13:55:50 2016
\begin{table}[htbp]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & $x'=0$ & $x'=1$ & $x'=2$ & $x'=3$ & $x'=4$ \\ 
  \hline
$x=0$ & 0.8379 & 0.1156 & 0.0347 & 0.0099 & 0.0019 \\ 
  $x=1$ & 0.3756 & 0.3005 & 0.1932 & 0.0985 & 0.0322 \\ 
  $x=2$ & 0.1352 & 0.2318 & 0.2659 & 0.2318 & 0.1352 \\ 
  $x=3$ & 0.0322 & 0.0985 & 0.1932 & 0.3005 & 0.3756 \\ 
  $x=4$ & 0.0019 & 0.0099 & 0.0347 & 0.1156 & 0.8379 \\ 
   \hline
\end{tabular}
\caption[Markov chain transition matrix for the Bayesian Iterated Learning model with sampling from the posterior]{Markov chain transition matrix for the Bayesian Iterated Learning model with $N=4$ and $\alpha/2=0.25$. The rows represent the probabilities of producing any of the given samples, assuming that the production is sampled from the posterior.} 
\label{tbl:transitionmatrixsample}
\end{table}
% latex table generated in R 3.2.3 by xtable 1.8-2 package
% Thu Aug 25 13:55:50 2016
\begin{table}[htbp]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & $x'=0$ & $x'=1$ & $x'=2$ & $x'=3$ & $x'=4$ \\ 
  \hline
$x=0$ & 1.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0000 \\ 
  $x=1$ & 0.6561 & 0.2916 & 0.0486 & 0.0036 & 0.0001 \\ 
  $x=2$ & 0.0625 & 0.2500 & 0.3750 & 0.2500 & 0.0625 \\ 
  $x=3$ & 0.0001 & 0.0036 & 0.0486 & 0.2916 & 0.6561 \\ 
  $x=4$ & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 1.0000 \\ 
   \hline
\end{tabular}
\caption[Markov chain transition matrix for the Bayesian Iterated Learning model selecting the mean of the posterior]{Markov chain transition matrix for the Bayesian Iterated Learning model with $N=4$ and $\alpha/2=0.25$. The rows represent the probabilities of producing any of the given samples, equivalent to $Bin(x';N,p=\theta_{MAP})$} 
\label{tbl:transitionmatrixmap}
\end{table}


The system that we describe by specifying the transition probabilities between individual states is a random process called a \emph{Markov chain}. Stochastic systems of this kind are said to obey the \emph{Markov property}, which means that the probability of entering a particular state only depends on the system's \emph{current} state, but not on any other prior states or state sequences that preceded the current one. This image of a \emph{chain} maps neatly onto the Iterated Learning model, where every new learner receives input from their parent generation who they then replace.

Importantly for us, the characterisation of a stochastic system as a Markov chain allows for straightforward analyses of different kinds. For example, assuming that our system would run indefinitely, we can calculate the probability of this infinite chain of states to occupy a particular state \emph{in the limit}. The so-called \emph{stationary distribution}~$\pi$ of a Markov chain transition matrix~$T$ is a probability distribution over its states~$S$, i.e.~it must satisfy
\begin{equation}
\pi\ge0,\; \sum_{s\in S} \pi_s = 1\;.
\end{equation}

In mathematical terms, the stationary distribution has the property that performing another iteration of the chain must map the distribution onto itself, i.e.\index{stationary distribution}

\begin{equation}\label{eq:stationarydistribution}
\pi = \pi\cdot T\;.
\end{equation}

Based on these definitions, it is possible for a given Markov chain to have more than one stationary distribution. This is generally only the case when the state space consists of subpartitions that cannot be reached from each other, as is the case when there is more than one absorbing state. The stationary distributions of the different systems whose input/mean-output mapping we visualised previously in Figure~\ref{fig:meanmapping} are shown in Figure~\ref{fig:stationarydistribution}.

\begin{figure}[htbp]

{\centering \subfloat[Stationary distribution for chains of learners who are sampling from the posterior.\label{fig:stationarydistribution1}]{\includegraphics[width=\maxwidth]{figure/stationarydistribution-1} }
\subfloat[average\label{fig:averagerstationarydistribution}\label{fig:stationarydistribution2}]{\includegraphics[width=\maxwidth]{figure/stationarydistribution-2} }
\subfloat[Stationary distribution for \emph{maximum a posteriori}~(MAP) learners. The different colours indicate that for $\alpha/2\le 1$ the Markov chain has two absorbing states, corresponding to categorical usage of either of the variants.\label{fig:stationarydistribution3}]{\includegraphics[width=\maxwidth]{figure/stationarydistribution-3} }

}

\caption[Stationary distributions of the Markov chain transition matrices]{Stationary distributions of the Markov chain transition matrices.}\label{fig:stationarydistribution}
\end{figure}



The stationary distributions confirm that the parameter~$\alpha$ indeed works as intended: when $\alpha/2<1$, the chains spend most of their time in the extreme states corresponding to categorical usage of either of the two competing variants. %As the sample size $N$ increases 
When $\alpha/2>1$, on the other hand, the chains mostly consist of learners who mix the variants evenly. The behaviour with intermediate values $\alpha/2\approx1$ falls in between, with the exact distribution also depending on the type of learners.

The MAP learner, not considered in the original \citeauthor{Reali2009} papers, deserves special attention: as already hinted at above, only this learning strategy looks like a proper \emph{regulariser} in the sense that an input proportion will, \emph{on average}, result in an output proportion that is in fact more regular than the input. It is also the only learning strategy which, for any $\alpha/2\le1$, does not introduce variation when there isn't any in the input, i.e.~learners who receive homogeneous input will never spontaneously introduce variation into their output. Figure~\ref{fig:stationarydistribution3} shows that, as a consequence, chains of such learners will end up in either of two \emph{absorbing states} corresponding to categorical usage of a variant, and remain there indefinitely.

\subsection{Neutral evolution and s-shaped curves}

So far our analysis of the stationary distribution limits us to describing the expected state of a model, but abstracted away from time.
One particular claim of \citet{Reali2010} concerning temporal dynamics is that even the neutral evolution model described by Bayesian regularisers will produce s-shaped curves. While we would not expect completely symmetric replication such as produced by neutral evolution to produce particularly directed transitions, they argue that this depends on which data is considered. In particular, since historical linguists only (or at least primarily) describe changes which have gone to completion, our assessment of whether a model produces s-shaped curves should equally be limited to data of this kind. They consequently go on to analyse only those chains that start off in a state where the first generation uses one of the competing variants categorically, while the last ends up in the opposite state where its productions contain only the other variant.

In order to get a better understanding of the underlying dynamics of our Markov model, we will therefore need to switch to an analysis that allows us to condition the Markov chains to be in specific states at specific points in time. One tool to do exactly this are \emph{Hidden Markov Models}~(HMMs). As the name suggests, HMMs are closely related to the Markov models described above. While in `normal' Markov chains the state sequence is directly visible to the observer, Hidden Markov Models allow us to specify a certain level of uncertainty over the model's state at any given point in time. Of particular importance to is that, instead of just randomly generating state sequences, HMMs allow us to make probabilistic inferences about the most likely states or state sequences that our model is likely to be in.

In what follows, I used R's \texttt{HMM} package~\citep{HMM1.0} to both replicate and extend the results reported in~\citet{Reali2010}. Firstly, Figure~\ref{fig:naiveconditioning} shows a replication of the original analysis from their paper. All four subplots show the state probability distribution for Markov chains of length $50$ where the input data presented to the first generation consisted of $50$ instances of only one variant. The probability distribution is represented as a heat map where, for any specific generation, darker colors indicate a higher probability of being in a state at that time. The probabilities of all states per generation sum to 1. The particular probability distributions shown here were calculated for chains of learners which use the inferred mean $\hat{\theta}$ of the posterior distribution to sample data for the following generation, but results for learners sampling directly from the posterior are qualitatively similar.

\begin{figure}[htbp]

{\centering \subfloat[Results with learners accepting the \emph{mean} of the posterior as their hypothesis for~$\theta$ with $\alpha=0.5$.\label{fig:naiveconditioning1}]{\includegraphics[width=\maxwidth]{figure/naiveconditioning-1} }
\subfloat[Results with learners accepting the \emph{mean} of the posterior as their hypothesis for~$\theta$ with $\alpha=10$.\label{fig:naiveconditioning2}]{\includegraphics[width=\maxwidth]{figure/naiveconditioning-2} }

}

\caption[State probability distribution for all Markov chains where the input to the first generation consists of tokens of only one variant]{State probability distribution for all Markov chains of length 50 where the input to the first generation consists of tokens of only one variant. The dashed white line indicates the trajectory through the `average' states that the chain is in at any given point in time. \emph{(i)}~conditioning on the first generation's input only \emph{(ii)}~conditioning on both the first and final generations' data.}\label{fig:naiveconditioning}
\end{figure}



Subfigures~(i) on the left show the development of the chains when conditioning on this initial state only. These two plots, which differ only in their setting of~$\alpha$, neatly highlight the contrast between the two different regimes of the regularisation parameter $alpha$: in Figure~\ref{fig:naiveconditioning1} we set $\alpha=0.5$, corresponding to \emph{regularisation}. In this setting, chains of learners are drawn to produce either of the two variants (near-)categorically. Note that, even though the system starts off with only one variant as its input, the chance introduction of tokens of the competing variant leads some chains to eventually regularise in the `other direction': whenever tokens of the other variant accumulate through random sampling, the chains start to be equally drawn towards the other fully regular state, i.e.~categorical usage of the formerly unattested variant.

While even after 50~generations the majority of chains is still at or near the usage frequency that was presented to the first participant, increasingly chains will start to `bunch up' against the top-most state corresponding to categorical usage of the other variant. Indeed, in the limit we should expect the the right-most `slice' of Figure~\ref{fig:naiveconditioning1}(i) to become completely symmetric around the halfway-mark, as it approaches the Markov chain's stationary distribution shown above in Figure~\ref{fig:stationarydistribution2}(i).

In contrast, the left panel of Figure~\ref{fig:naiveconditioning2} with $\alpha=10$ represents the \emph{de-regularisation} regime, where individuals prefer to use both variants equally. This is borne out by the fact that chains of such learners are quickly drawn towards the middle states, indicating mixed usage.

Subfigures~(ii) on the right-hand side show the expected distribution of states when conditioning on both the initial and final states of the chain, where the last individual only produces tokens of the competing variant that was not attested in the first generation's input data. While the probability distribution over possible states at most intermediate generations is extremely wide, \citeauthor{Reali2010} point to the \emph{average trajectory}~(shown in white) that is calculated by computing the \emph{average state} of all chains at any given generation. They point out that, intriguingly, the shape of this average trajectory is dependent on the regularisation parameter $\alpha$. In particular, the model produces s-shaped trajectories exactly when chains are geared towards regularising input, which experimental evidence suggests is in fact a feature of human language learning~\citep{HudsonKam2005,Reali2009,Smith2010}.

It is crucial to point out here is that this \emph{average} of all transitions is not necessarily representative of the model's \emph{typical} transitions~\citep{Blythe2012neutral}. In order to get an idea of what individual trajectories of Iterated Learning chains actually look like, we can simply generate state sequences of the underlying Markov model randomly and filter them according to the start and end conditions~(see Appendix~\ref{app:markovmodel} for the code).

Figure~\ref{fig:naiveconditioningtransitions} shows three randomly generated chains that fulfill both the start and end condition specified above. The trajectories were generated using exactly the same parameter setting as the one underlying the s-shaped average trajectory shown in Figure~\ref{fig:naiveconditioning}. Already here we can see that individual trajectories are much more noisy, less directed and s-shaped than the numerically computed `average transition' above suggests.

\begin{figure}[htbp]

{\centering \includegraphics[width=\maxwidth]{figure/naiveconditioningtransitions-1} 

}

\caption[Three randomly generated Markov chains initiated at $0/N$ and terminating at $N/N$ after 50 iterations.]{Three randomly generated Markov chains initiated at $0/N$ and terminating at $N/N$ after 50 iterations. \emph{(i)}~learners sampling from the posterior distribution~$p(\theta|x)$. \emph{(ii)}~learners accepting the \emph{mean} of the posterior as their hypothesis for~$\theta$.}\label{fig:naiveconditioningtransitions}
\end{figure}



What is also evident is that not all of the `transitions' are actually of the length that we specified: many chains either remain at the initial state for some time, or otherwise converge on categorical usage of the other variant early and remain there until the remaining generations have passed. This points to another more general problem, namely that termination after exactly 50~generations is not actually well-motivated. %Even a single-generation jump from from 0/50 to 50/50 has a non-zero~(if extremely small) probability and would arguably not be s-shaped.
To understand the dynamics of this model even better we should therefore take a closer look at the expected duration of transitions.

\subsubsection{Expected number of generations for a transition to complete}

In order to get a more accurate picture of the typical trajectory exhibited by regularising Iterated Leaners, we first need to know the likelihood of a transition completing in a given number of generations. Figures~\ref{fig:naiveconditioningprobabilities} shows both the per-iteration probability as well as the cumulative probability of a chain of Iterated Learners reaching categorical usage of the initially non-existent, incoming variant over time.

\begin{figure}[htbp]

{\centering \includegraphics[width=\maxwidth]{figure/naiveconditioningprobabilities-1} 

}

\caption[Probability of transitions from categorical usage of one to categorical usage of the other variant]{Probability of transitions from categorical usage of one to categorical usage of the other variant, for learners accepting the \emph{mean} of the posterior as their hypothesis for~$\theta$. \emph{(i)}~probability of completing first transition after the given number of generations \emph{(ii)}~cumulative probability of having completed at least one transition.}\label{fig:naiveconditioningprobabilities}
\end{figure}



For the \emph{averaging} learner with parameters $\frac{\alpha}{2}=0.25$ and $N=50$ as above, the chain is most likely to first reach categorical usage of the incoming variant at the distribution's mode after 149 generations, while on average the first transition takes 444 iterations to complete.

The distribution of the expected duration of a transition by a chain of learners sampling directly from the posterior distribution is qualitatively similar. Using the same parameter settings as above, the most likely and mean duration until completion of the first transition are 87 and 310 respectively).

%Knowing this we can now generate some of these more `typical' trajectories.

\subsubsection{Average trajectory of transitions that have the exact same duration}

As pointed out above, the number of generations until a new variant has fixated isn't actually representative of the \emph{duration} of a transition. Since chains might remain at their initial state for a few iterations before picking up, or also return back to the initial state before picking up again. If we are interested in the length of the actual transition (i.e.~we only start to measure the duration of a transition when the new variant is first innovated) the distribution of transition durations looks quite different, as shown in Figure~\ref{fig:complexconditioningprobabilities}.

\begin{figure}[htbp]

{\centering \subfloat[Results with learners sampling from the posterior distribution~$p(\theta|x)$.\label{fig:complexconditioningprobabilities1}]{\includegraphics[width=\maxwidth]{figure/complexconditioningprobabilities-1} }
\subfloat[Results with learners accepting the \emph{mean} of the posterior as their hypothesis for~$\theta$.\label{fig:complexconditioningprobabilities2}]{\includegraphics[width=\maxwidth]{figure/complexconditioningprobabilities-2} }

}

\caption[Probability of having completed a transition in exactly the number of generations without ever reverting back to the initial state]{Probability of having completed a transition in exactly the number of generations without ever reverting back to the initial state.}\label{fig:complexconditioningprobabilities}
\end{figure}



An immediately obvious difference between this and the earlier distribution of transition durations in Figure~\ref{fig:naiveconditioningprobabilities} is that the cumulative probability in subfigure~(i) never reaches 1. Under \citet{Reali2010}'s original condition on the final state only, which allowed all possible intermediate trajectories, all chains would eventually reach the target state at some point.

Not so when conditioning on transitions which have to last an exact number of generations: Figure~\ref{fig:complexconditioningprobabilities} only considers transitions that, from their moment of actuation, actually reach the target state without ever `failing' (i.e.~returning back to the categorical initial state) in between. For a chain of learners who take the mean of their posterior distribution as their hypothesised underlying frequency~$\theta$, only about $1.46\%$ of initial introductions of a new, competing variant actually lead to successful transitions without any interruptions.

In terms of the distribution of durations of those transitions which \emph{are} successful, the number of generations until completion are expectedly much lower than in Figure~\ref{fig:naiveconditioningprobabilities} above. For the \emph{averaging} learner, the most like exact duration of a successful transition is much lower at 75~generations, with the mean duration at around~135 generations.
For the \emph{sampler} the values are even lower~(mode 36, mean~58).

Figure~\ref{fig:doubleconditioned} shows the state probability distribution as well as average trajectory of the Markov chains which are conditioned on introducing the initially unattested variant in the very first generation, as well as on only tokens of that variant at the maximum number of generations (and no earlier), without ever returning to the initial state.
Results are shown for both \emph{sampling}~(Figure~\ref{fig:doubleconditioned1}) as well as \emph{averaging}~(Figure~\ref{fig:doubleconditioned2}) learners for two different representative durations, the most likely duration of a transition~(the mode of the distributions in Figure~\ref{fig:complexconditioningprobabilities}) and the~(higher) \emph{mean} duration. %for a transition using the given parameters)
The Figure shows that the average of all transitions, again indicated by the dashed white line, is actually more like an~$S$ bent in the `wrong' direction. In other words, unlike what we find in empirical data on language changes, some of the slowest rates of growth occur at the mid-point of the change, similar to the average transition of chains of \emph{de-regularising} learners shown in Figure~\ref{fig:naiveconditioning2}.

\begin{figure}[htbp]

{\centering \subfloat[Results with learners sampling from the posterior distribution~$p(\theta|x)$, most likely duration and average duration of a transition are 36 and 58 generations, respectively.\label{fig:doubleconditioned1}]{\includegraphics[width=\maxwidth]{figure/doubleconditioned-1} }
\subfloat[Results with learners accepting the \emph{mean} of the posterior as their hypothesis for~$\theta$, most likely duration and average duration of a transition are 75 and 135 generations, respectively.\label{fig:doubleconditioned2}]{\includegraphics[width=\maxwidth]{figure/doubleconditioned-2} }

}

\caption[State probability distribution for all Markov chains exhibiting a transition with the exact same duration.]{State probability distribution for all Markov chains exhibiting a transition with the exact same duration. The dashed white line shows the \emph{average} trajectory, while the white dots indicate one of the \emph{most likely} transition paths. The duration is set to be equal to \textit{(i)}~the most likely duration of a transition and \textit{(ii)}~the average duration of all completed transitions respectively, as computed for parameters $N=50, \alpha=0.5$.}\label{fig:doubleconditioned}
\end{figure}



What Figure~\ref{fig:doubleconditioned} also shows up, however, is that even using this arguably more accurate conditioning on exact start and end points of the transitions as well as on a more realistic time scale, the average trajectory is still not an accurate representation of a typical trajectory. Marked by the white dots is one of the \emph{most likely} individual trajectories that the Markov chain passes through on its way from the initial to the final state. This trajectory is determined using the \emph{Viterbi algorithm}~\citep{Jurafsky2008}, a dynamic programming algorithm for Hidden Markov Models that allows one to infer the most likely sequence of states given a sequence of observations which only reveal partial information about the likely underlying states. The algorithm can be used for our purposes by providing it with a sequence of observations that indicate categorical usage of one variant at the start and categorical usage of the other at the end of the sequence, with a fixed number of observations representing an unspecified degree of mixed usage in between~(the source code as well as a more detailed description of the approach can be found in Appendix~\ref{app:markovmodel}).
The sequence of underlying state transitions which has the highest overall likelihood of all possible paths given these observations is one that rapidly crosses the mixed-usage area in 10-15~generations, and remains hovering at near-categorical usage of either variant for the rest of the time. (It should be noted that the exact position of this fast transition along the time axis is irrelevant, in fact all transitions \emph{parallel} to the one indicated by the dots, i.e.~ones with the same shape but actuating at earlier or later generations, have the exact same probability of occurring.)

%Even the average of these more strictly conditioned trajectories doesn't look particularly s-shaped anymore. Moreover, while shorter transitions (like ones of the length of the mode, left figure) still have the fastest rate of change at the mid-point, \emph{longer} 

To finish our study of the individual transitions generation by this model, we randomly generate a final set of transitions, limiting ourselves to only those that first complete after \emph{exactly} the specific number of generations, i.e.~we exclude ones that reach a frequency of~50 of the incoming variant early and stay there. Three such example transitions can be seen in Figure~\ref{fig:complexconditioningtransitions}.

\begin{figure}[htbp]

{\centering \subfloat[Results with learners sampling from the posterior distribution~$p(\theta|x)$.\label{fig:complexconditioningtransitions1}]{\includegraphics[width=\maxwidth]{figure/complexconditioningtransitions-1} }
\subfloat[Results with learners accepting the \emph{mean} of the posterior as their hypothesis for~$\theta$.\label{fig:complexconditioningtransitions2}]{\includegraphics[width=\maxwidth]{figure/complexconditioningtransitions-2} }

}

\caption[Three randomly generated transitions which first exhibit categorical usage of the new variant exactly after the average number of generations it takes a chain to complete a transition]{Three randomly generated transitions which first exhibit categorical usage of the new variant exactly after the average number of generations it takes a chain to complete a transition. The duration of transitions is equal to \emph{(i)}~the most likely and \emph{(ii)}~the \emph{average} duration of a transition given the parameter settings~($N=50, \alpha=0.5$).}\label{fig:complexconditioningtransitions}
\end{figure}



\subsection{Effect of sample size on the duration of transitions}

No matter what the shape of the average trajectory might be, for the sake of cross-validating the general results of the neutral evolution models as implemented here as well as by the Utterance Selection Model, we can compare the two models' predictions regarding how the expected duration of transitions develops as a function of the `population size'.

While in the Utterance Selection Model the `population size' refers explicitly to the size of the \emph{speech community}~(i.e. it is a measure of the number of interacting individuals), \citeauthor{Reali2009} are more implicit about the precise meaning of their model parameter~$N$.
In~\citet{Reali2010} they show that a chain of learners employing a specific sample size~$N$ that accepts the average of the posterior as their hypothesis for the underlying frequency~$\theta$ is identical to the Wright-Fisher model of neutral evolution with symmetric mutation rates, an equivalence that will be discussed more in-depth in Section~\ref{sec:realigriffithsequivalence}.
Taking the equivalence of these two models literally would mean that the parameter~$N$ in the present model corresponded to the population size of a group of Bayesian learners, each of which uses either of the variants categorically, with the probability of adopting either variant given by~$\hat{\theta}$.
% TODO unpack

Another way to construe the meaning of parameter~$N$ corresponds to how it is mapped onto an Iterated Learning experiment on humans in~\citet{Reali2009}. Here, the model is fit to a chain of single individuals, each of which first receives and then produces a sample of $N$~tokens. While not exactly specifying a feature of the individual, the function that $N$ fulfills in this context is to control the resolution at which the data is presented to and produced by individual participants in the chain. In this sense, the parameter fulfills a function very similar to the $T$ parameter of the Utterance Selection Model described above.

On the other hand, the fact that the model does not allow for continuous updating of the internal representations once they are acquired, but is instead based on a one-time learning event of sample size~$N$, means that the set of possible posterior distributions~$p(\theta|x)$, as well as the resolution of possible values of $\theta$ for strategies that adopt one value deterministically, is completely constrained by~$N$. % The size of learning outcomes is finite / But, due to the one-off nature of learning in an Iterated Learning model
As a consequence, the parameter inadvertently acts as something like a \emph{memory capacity} of the individual which, unlike the USM's sample resolution~$T$, also limits the individual agents' representational resolution of the frequency distribution they are trying to acquire. % a strange convolution of variable frequency, memory capacity

\begin{figure}[htbp]

{\centering \includegraphics[width=\maxwidth]{figure/realipopulationsize-1} 

}

\caption[Mean and mode of the duration of transitions as a function of the parameter~]{Mean and mode of the duration of transitions as a function of the parameter~$N$, with $\alpha=0.5$.}\label{fig:realipopulationsize}
\end{figure}



Whichever way the parameter is to be construed, its setting does not just affect the likelihood of transitions occurring, but also the transitions' duration and shape. The parameter's effect on the average as well as most likely \emph{duration} of completed transitions in chains of learners is shown in Figure~\ref{fig:realipopulationsize}. In all cases, $N$~shows a linear relationship with the time until fixation for all measures with varying slopes, a result that is in line with findings for expected diffusion times obtained from other general models of neutral evolution~\citep{Kimura1969}. %Blythe2007divided

%Even under this very different type of model we can reproduce the results from the neutral evolution regime of the Utterance Selection Model as discussed in Section~\ref{sec:usmneutral}: the duration of transitions from (near)-categorical usage of one variant to another under neutral evolution, where neither variant has any selection bias acting in their favour, increases linearly with the size of the population.

\subsection{Summary}

To complement the study of various different replication regimes implement in the USM framework earlier, I presented a replication of \citeauthor{Reali2009}'s Markov model of neutral evolution with symmetric innovation, a model that has been used to make concrete claims about the possibility of s-shaped transitions in the absence of asymmetry between variants~\citep{Reali2010}.
However, neither the dynamics of individual transitions, nor a closer investigation of the \emph{average} trajectories under different conditioning assumptions suggests that this model of neutral evolution based on regularising Bayesian learners exhibits curves that are particularly directed, instead producing noisy transitions with frequent reversals and restarts.
Also, in agreement with other models of neutral evolution, the expected duration of a transition from categorical use of one variant to categorical use of another increases linearly with the population/memory size parameter.
Another important conclusion regarding modelling more generally is that, when one is interested in the \emph{temporal dynamics} of a system it is indispensable to look not only at the end states or average dynamics as a shortcut, but that a more exhaustive analysis of the actual dynamics and \emph{typical} transitions is required.
