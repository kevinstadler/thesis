\section{The Utterance Selection Model}\index{Utterance Selection Model}
\label{sec:usm}

%\section{Comparing accounts using the Utterance Selection Model}
%Introduce USM generally, including EWMA-formulation and general learning/alignment properties

The version of the Utterance Selection Model~(USM) discussed here grew out of \citeauthor{Croft2000}'s more general formulation of language change as evolutionary competition between utterances. While in its original, theoretical formulation in~\citet{Croft2000} it is truly full \emph{utterances} which are undergoing replication, in its mathematical-computational incarnation the USM is best understood as a quantitative model of the competition between different variants of one sociolinguistic variable, as described in Section~\ref{sec:sociolinguisticvariable}.\index{sociolinguistic variable}

At its core, every agent in the USM is completely characterised by its variable use over variants, specified by the proportions with which each variant is used, all of which together sum to~1. For sake of simplicity we will limit ourselves to the canonical case of two competing variants, where the behaviour of an agent~$i$ can be captured by a single variable~$0<x_i<1$ representing its relative usage level of the incoming variant, with that of the competing variant taken to be~$1-x_i$.

The primary contribution of the computational USM is that it provides a well-defined and rich framework to study the dynamics of these internal usage levels as they are influenced by observing realisations of the same linguistic variable in interactions with other speakers in a population. % TODO The USM 

\subsection{Model parameters of the USM}

\subsubsection{Learning rate $\lambda$}

Following an interaction, the agents update their internal frequency according to the following USM update rule, which is again applied for both agents~\citep[p.4]{Baxter2006}:
%\cite[p.4]{Baxter2006}:
\begin{equation}\label{eq:usm1}
x_i' = \frac{x_i+\lambda\cdot y_i}{1+\lambda}\;,
\end{equation}
where $y_i$ is the subjective \emph{perceived frequency} of the variable usage rate, whose computation will be discussed below.

%$$x_i' = \frac{x_i+\lambda((1-H_{ij})n_i+H_{ij}n_j)}{1+\lambda} = \frac{x_i}{1+\lambda} + \frac{\lambda(\dots)}{1+\lambda}$$% H_{ij} \in [0,1]$$
%$$H_{ij} = \lambda h$$

Perhaps the most important model parameter is the agents' learning rate~$\lambda$, which is by default assumed to be the same for all agents. What the USM's update rule in Equation~\ref{eq:usm1} does is change an agent's internal frequency~$x_i$ by shifting it a small step towards the relative perceived frequency that it observed in its most recent interaction.
The higher the learning rate, the larger the step towards this target frequency: at~$\lambda=0$ there is no learning and the agent remains at their initial frequency forever, as~$\lambda\rightarrow\infty$, the agent approaches a regime in which they instantly adopt exactly those usage frequencies observed in their last interaction.
While there are instantiations of the USM in which the learning rate for individual agents is not constant but \emph{decreases} over time to imitate the effect of increasing rigidity of language use with age~\citep{Baxter2016}, this thesis will be concerned with the simpler case of a constant learning rate that is identical for all agents in the population.
Since we are mostly interested in reliable model behaviour that exhibits gradual assimilation rather than abrupt and erratic changes in individual usage levels, like most investigations of the USM we will limit ourselves to low values in the range of~$\lambda\le0.01$).

It should be acknowledged that the particular form of the learning rule was partly chosen due to its mathematical properties, which make it amenable to analysis using tools from statistical physics~\citep[see in particular][]{Baxter2006}. To get a more intuitive understanding of what the update rule does in terms of agents' learning dynamics, it is worth noting that it is equivalent to defining an agent's usage levels as an exponentially weighted moving average~(EWMA) over its learning input data series of perceived frequencies~$\vec{y}$.
EWMAs themselves are a generalisation of Bush-Mosteller learning~\citep{Bush1955} for non-discrete input data points which, rather than employing a fixed time window to average over, always gives relatively more weight to the most recent data points, with the absolute contributions of individual learning samples decaying over time.
Upon receiving a new data point~$y$ indicating a certain usage level observed in an interaction, the agent updates their own usage level~$x$ according to

\begin{equation}
x' = (1-\alpha)\cdot x + \alpha\cdot y\;.
\end{equation}

This representation of the learning rule makes it clear that the agent's own usage level is simply a moving overage over the perceived frequencies it observes in interactions, where $\alpha$ controls the relative weight of the newest data point toward that moving average.
%At time $t$ datum $y_{t-i}$ has weight $\alpha(1-\alpha)^{i-1}$,
This formulation is equivalent to the original USM updating rule in Equation~\ref{eq:usm1} given

\begin{equation}
\alpha = \frac{\lambda}{1+\lambda}
\end{equation}
\begin{equation}
\lambda = \frac{\alpha}{1-\alpha}\;,
\end{equation}
the only difference being a rescaling of the parameter space from $\lambda\in[0,\infty)$ to $\alpha\in[0,1]$, as shown in Figure~\ref{fig:mapping}.



\begin{figure}[htbp]

{\centering \includegraphics[width=\maxwidth]{figure/mapping-1} 

}

\caption[Mapping between the two USM learning rate parameter spaces]{Mapping between the $\alpha$ and $\lambda$ parameter spaces, $\alpha = \frac{\lambda}{1+\lambda}$ or $\lambda = \frac{\alpha}{1-\alpha}$, respectively. $\lambda = 0$ corresponds to $\alpha = 0$, $\lambda = 1$ to $\alpha = 0.5$, and $\alpha = 1$ to the limit of $\lambda\rightarrow\infty$.}\label{fig:mapping}
\end{figure}



The USM's dynamics beyond the simple update rule are controlled by a number of other parameters which will be briefly introduced here, before their individual effects are explained in more detail in the following Sections.
Firstly, at every point in time a new pair of distinct agents~$i, j$ has to be chosen from the population, which consists of a fixed number of~$N$ agents total. Interacting agents are randomly drawn based on a matrix~$G$ which specifies the probabilities of interacting for all pairs of agents. 
Whenever an agent~$i$ with an internal frequency of use~$x_i$ is chosen to engage in an interaction with another speaker~$j$, they each produce and exchange~$T$ tokens of the variable under investigation by taking a sample from the corresponding Binomial distributions~$Bin(T, x_i)$ and~$Bin(T, x_j)$ respectively. Based on the samples~$n_i$ and~$n_j$ taken from each of the distributions, the agents combine the relative frequencies~$\frac{n_i}{T}$ and~$\frac{n_j}{T}$ into \emph{perceived frequencies}~$y_i, y_j$ according to the following formula:

\begin{equation}\label{eq:perceivedfrequency}
y_i = (1-H_{ij})\cdot f(\frac{n_i}{T}) + H_{ij}\cdot f(\frac{n_j}{T})\;.
\end{equation}

In other words, the perceived frequency is based on a weighted sum of the agent's own productions and that of their interlocutors, and is calculated separately for the other agent~$j$ by exchanging all the indices~$i, j$.
Here the high degree of modularity of the model becomes evident in the number of parameters, only some of which will be of interest to us here, but which it is worth going through in turn.

\subsubsection{Population size $N$}\index{production-perception loop}

Like virtually all models of language change, the USM is a \emph{multi}-agent model, i.e.~it simulates a \emph{population} of agents that engages in interactions. While a dynamic population with changing population size would be possible, most investigations are limited to assuming a fixed number of agents~$N$ that remain in the population the entire time~\citep[again see][for an exception]{Baxter2016}. This simplifying assumption lends the USM to more general analysis and enables to connect it to evolutionary models from other domains. In particular, \citet{Blythe2007divided} showed the USM's equivalence to \citeauthor{Wright1931}'s island model~\citeyearpar{Wright1931}, where the population size~$N$ corresponds to the number of biological subpopulations or `islands' between which only limited exchange of replicators takes place. The effect of different values of~$N$ on the dynamics of the USM depend on several of the other model parameters, and will be explored in more detail below.

\subsubsection{Social network structure/interaction probability matrix $G$}

The parameter~$G$ is a square matrix of size~$N\times N$ which specifies the probabilities for every pair of agents to be chosen to interact with each other, so that the sum over \emph{distinct} pairs $\sum_{\langle i,j\rangle}G_{ij}=1$. This parameter can not just gradually alter the frequency or density of interactions between different agents or agent groups. By setting a specific~$G_{ij}=0$ one can completely `disconnect' two agents $i,j$ in the interaction network, thereby creating the same effect that \emph{social network structure} has in many other multi-agent models of language change.
As I discussed above, the exact role that networks of social interactions have on the diffusion of language changes is still debated, with equally conflicting results over whether network structure matters fundamentally~\citep{Blythe2007divided,Fagyal2010,Gong2012,Pierrehumbert2014,Kauhanen2016} or only marginally~\citep{Nettle1999,Baxter2008,Blythe2009,Stadler2009}, with the results obtained from computational models again largely dependent on many other underlying assumptions and the particular learning models used.

Since this thesis will not investigate the effect of either network structures or nonuniform interaction probabilities, we will abdicate the many degrees of freedom bestowed by the this parameter matrix by always assuming a fully connected network of~$N$ agents with equal interaction probabilities, setting $G_{ij}=\frac{1}{N-1}$ for all~$i\ne j$.

\subsubsection{Accommodation/alignment matrix $H$}

The parameter~$H$ in Equation~\ref{eq:perceivedfrequency} above is a square matrix which specifies the weights that all ordered pairs of individual agents give to each others' productions, with $H_{ij} \in [0,1]$. At the extreme of $H_{ij}=0$, agent~$i$ completely discards any input it receives from agent~$j$ and its perceived frequency~$y_i$ is consequently completely determined by its own productions.
A value of $H_{ij}=0.5$ would give equal weight to both the speaker's and the listener's production in an interaction. By employing different values in the cells of $H$~(in particular by setting pairs of agents' mutual accommodation parameters $H_{ij}, H_{ji}$ unequal), the matrix can be used to model asymmetries in adoption structures in a population, as well as increased influence of some individuals' usage levels as a form of individual~(rather than variant) prestige, a mechanism that will be explored below.

Beyond using~$H$ to introduce individual differences, it is also possible to set uniform accommodation behaviour by setting all matrix values $H_{ij}$ to the same fixed constant~$h\in[0,1]$. The degree of accommodation only affects the USM's dynamics when there are systematic differences in usage levels within the population, which could be due to inter-individual differences such as age-stratified populations or differing variant selection biases~\citep{Baxter2016} or otherwise due to clusters or differences in the degree of connectivity in a social network, cases which have only seen limited investigation so far~\citep{Blythe2007divided,Michaud2017}.
Since this thesis will not be concerned with inter-individual differences or stratified network structures, all simulations will be performed so that agents are set to only align with their interlocutor and not to their own productions, equivalent to~$h=H_{ij}=1$ for all~$i,j$.

\subsubsection{Production sample resolution $T$}

$T$, a positive integer, is the aforementioned sample size which determines the `resolution' with which agents can observe the variable use of different variants of an agent with usage rate $x$ in an interaction by randomly sampling from a binomial distribution~$Bin(T,x)$. For sake of simplicity we will only be concerned with the case of two competing variants, but the definition generalises to~$k\ge3$ variants in which samples are taken from the multinomial distribution~$Mult(T,\vec{x})$, where an agent's usage probabilities over the $k$~variants are specified by a vector~$\vec{x}$ of length~$k-1$.

The parameter $T$ is rather unusual, in the sense that no comparable parameter features in most other computational models of language change. Among the many models referenced above, most can be assigned to one of two groups based on when and how learning, in the sense of inferring or updating a linguistic property or system from data, occurs. One group, in which agents remain in the population and learning occurs \emph{incrementally}, agents typically receive one data point at a time for each learning event they are sampled to partake in, for example in the Naming Game. In the other group of models there is explicit reference to a \emph{sample size} of the learning data, but there is typically only one learning event at the beginning of an agent's lifetime, such as in the case of the Iterated Learning model. %Moreover, in this case the sample size often refers to 
A combination of both, multiple learning events throughout an agent's life time each of which with a learning sample of more than one data point, is not normally considered, but turns out to be a crucial aspect of an evolutionary model of language change as described above. This has to do with the fact that the continued differential \emph{selection} of linguistic variants relies on the existence of variation in the population, variation which can only be attested in learning samples of sizes~$T>1$. This point will become more apparent when we discuss the core parameter that determines how agents derive the \emph{perceived} usage frequencies of variants in interactions, through the bias function~$f(.)$.

\subsubsection{Bias function $f(.)$}

While the numerical parameters so far all control some aspect of the population or interactions, the bias function~$f(.)$ is where \emph{selection} of specific variants comes into play. Its role is to alter the \emph{objective} relative frequency of tokens produced in the interaction, $\frac{n}{T}$, to an agent's subjective \emph{perceived} frequency.
$f(.)$ is simply a function that maps from the frequency interval~$(0,1)$ to~$(0,1)$. While in principle any arbitrary function could be plugged in here, most analyses are limited to mappings that obey some reasonable criteria, in particular that they are \emph{monotonically increasing} within the interval, so that relatively higher objective frequencies are always mapped to higher (or equal) perceived frequencies~\citep{Blythe2012}.

Special attention should be drawn to the fact that the bias function is only defined for~$(0,1)$ and that, per definition, $f(0)=0$ and $f(1)=1$. These two equivalences are imposed because~$f(.)$ embodies the \emph{differential selection} mechanism of the USM. While the bias function can alter the variation observed in individual samples in one way or another, the function must not indicate the presence of a variant when it is not attested in the sample. More generally, this constraint also stops the bias function from introducing new variants into the population, and thus a strict requirement for any evolutionary model that distinguishes the selection of existing variants from pressures of innovation through altered replication.

While the USM's original definition in~\citet{Baxter2006} also incorporated parameters for the spontanous generation of unattested variants, most studies of the model so far have been concerned with the analysis of the diffusion and selection of traits that are already established at a low level across the population. With the exception of the final chapter, this thesis will also primarily be concerned with \emph{selection} mechanisms, of which many different ones can be implemented through the function~$f(.)$.

%This function can be used to implement a mechanism of \emph{regularisation}, discussed in Section~\ref{sec:usmregularisation}, as well as \emph{replicator selection} mechanisms which prefer some of the competing variants over others, a case which will be discussed in Section~\ref{sec:usmreplicator}.
%and different functions for this parameter will be the main subject in the remaining sections.

%[Comparing accounts with the USM]
\section{Comparing accounts with the Utterance Selection Model}

Having covered the general mechanism of the USM, we can now investigate the predicted dynamics under the presence (or absence) of different biases. This section recapitulates the in-depth study of several different USM biases by~\citet{Blythe2012} while contributing an additional model of asymmetric replicator selection in Subsection~\ref{sec:usmreplicator}. The motivation for the present analysis is to address the question of which accounts or presumed pressures would predict s-shaped transitions of variant use (and under which conditions) when compared in one unified framework, which necessarily also includes a detailed study of the model's baseline behaviour in the absence of any pressures.

%Based on the general formulation of the utterance selection model presented above, \citet{Blythe2012} set out to address ongoing discussions on different social accounts of change to see whether the `mechanical view' and prestige 
\subsection{Neutral evolution}\index{neutral evolution}
\label{sec:usmneutral}

%Neutral evolution and the dynamics of the USM's minimal assumptions: interactions of $\lambda$, $x$, and new input $n$

While the USM's updating rule given in Equation~\ref{eq:usm1} is very general and allows for a vast number of modifications through the additional parameters, it is interesting to analyse the model's learning dynamics in the absence of any pressures of either innovation or differential replication.
This \emph{neutral evolution} condition, so-called because it is based on completely neutral replication of existing traits from the population according to their current prevalence~\citep{Blythe2012neutral}, is achieved by using the identity function
\begin{equation}
f(u) = u
\end{equation}
as the USM's bias function, which means that the agents' \emph{perceived} frequency~$y$ in an interaction can be directly derived from their interlocutor's productions, i.e.
\begin{equation}
y_i = \frac{n_j}{T}\;.
\end{equation}

Using this simple assumption, we can investigate when the agents' internal~$x$~value changes most. Since a lot of the dynamics stem from the basic learning rules, the exact roles of the basic parameters and their behaviour at different moments in the model should be studied in detail.
%For sake of thoroughness and to better understand the baseline dynamics of the updating rule, we can investigate the interaction between the agent's internal usage level~$x$ and how it changes after an interaction.
%Since a lot of the dynamics stem from the basic learning rules, the exact roles of the basic parameters and their behaviour at different moments in the model should be studied in detail.
Firstly, Figure~\ref{fig:singledxpern} shows the point change away from a internal usage proportion~$x=0$ for different input data points~$y$ as a function of the agent's learning rate~(plots are provided for both the $\alpha$ as well as the~$\lambda$ formulation of the learning rate).
The equal spacing between the curves for different~$y$ means that the impact of different input data points is proportional to their difference to the agent's internal value~$x$.

\begin{figure}[htbp]

{\centering \includegraphics[width=\maxwidth]{figure/singledxpern-1} 

}

\caption[Absolute point change to the agent's usage rate for different learning rates with the same initial value ]{Absolute point change to the agent's usage rate for different learning rates with the same initial value $x=0$ given different input datapoints~$y$. Left: absolute point change as a function of the learning rate~$\alpha$. Given an x value at one extreme and input data at the other, the maximum change to x is equal to $\alpha$. Right: absolute point change as a function of the learning rate~$\lambda$.}\label{fig:singledxpern}
\end{figure}



\begin{figure}[htbp]

{\centering \includegraphics[width=\maxwidth]{figure/singledxperx-1} 

}

\caption[Absolute changes for the same input data ]{Absolute changes for the same input data $y=1$ for different values of $x$ given a range of learning rates $\alpha$~(left) and $\lambda$~(right). Given an x value of $0$ at one extreme and input data $y=1$ at the other, the maximum change to x is equal to $\alpha$.}\label{fig:singledxperx}
\end{figure}



In fact, an identical picture emerges in the case of a fixed input data point~$y=1$ that is incorporated into different internal values~$x$, as shown in Figure~\ref{fig:singledxperx}. %Moreover, the straight lines What this tells us is that there is no interaction at all between the learning rate $\alpha$ and input data $y$, both of which have a strictly linear effect on the outcome of individual learning events given the same starting frequency~$x$.
Generally, given our EWMA update rule we find that
\begin{equation}\label{eq:usmdiff}
\Delta x = x'-x = \alpha\cdot n + (1-\alpha)\cdot x - x = \alpha\cdot(n-x)\;,
\end{equation}
i.e.~the point change to~$x$ is always directly proportional to the difference between the agent's current usage level~$x$ and the input data~$y$.
The USM's individual agent update dynamics therefore follow a general learning framework that is free from nonlinearities and which, in the absence of any biasing, has been shown to be equivalent to several other models of neutral evolution in biology~\citep{Blythe2007divided}, and whose dynamics are not significantly affected by setting the neutrally copying agents in many different types of structured networks~\citep[but see \citealt{Kauhanen2016}]{Blythe2010,Blythe2012copying}.
%, but how are the individual interactions and their resulting perceived frequencies~$y_i$ and~$y_j$ determined?

Due to the complete lack of asymmetries in the neutral evolution condition its underlying mechanism, often referred to as ``random copying'', has scarcely been proposed to be the underlying force of language change, or even cultural change more generally~\citep{Mesoudi2009}.
Upon numerical inspection, neutral evolution exhibits its characteristic dynamics which include ``large fluctuations and a tendency for an upward or downward trend to reverse one or more times before an innovative variant goes extinct or wins out''~\citep[p.285]{Blythe2012}.
\citet{Blythe2012neutral} in particular argues that neutral evolution should in fact be taken as a \emph{null model} against which competing accounts of language change should be compared, as a baseline similar to those underlying the neutral theory of molecular evolution in biology~\citep{Kimura1983}. Apart from using it as such a null condition for the model presented in Chapter~\ref{ch:momentummodel}, I will also return to an in-depth study of the dynamics of neutral evolution in Section~\ref{sec:realigriffiths}. But for now we will focus on replication mechanisms in the USM that actually implement \emph{selection} of some kind.

\subsection{Replicator-neutral selection}\index{regularisation}\index{selection}\label{sec:usmregularisation}
While the term selection is often associated with a preference for particular variants, it really covers \emph{differential replication} of any kind, and can therefore also be used to implement \emph{symmetric} selection functions which are neutral regarding the different variants or replicators. One example is the case of \emph{frequency-dependent selection} biases which can be used to systematically favour variants not based any inherent \emph{a priori} property but based on their current attested frequency in the population, a feature which itself changes over time~\citep{Boyd1985}. As the simplest symmetric, frequency-dependent selection mechanism \citet{Blythe2012} suggest the function

\begin{equation}\label{eq:usmregularisation}
f(u) = u + a\cdot u\cdot (1-u)\cdot(2u-1)\;,
\end{equation}
where values of the parameter~$a>0$ lead to a \emph{boost} of variants whose relative frequency is already greater than~$50\%$, while settings of~$a<0$ implement selection in favour of any variants currently in the minority.
The two regimes of this non-linear selection function are displayed in Figure~\ref{fig:usmregularisation1}, giving a visual sense of the function's symmetry: mirroring the plots along both axes yields an identical curve, an indication that, mathematically, the function satisfies the symmetry criterion $f(u) = 1 - f(1-u)$~\citep{Blythe2012}.

\begin{figure}[htbp]

{\centering \subfloat[The frequency-dependent selection mapping function from Equation~\ref{eq:usmregularisation} against the baseline of neutral evolution, $f(u)=u$, indicated by the dotted line. \emph{(i)}~conformity copying with $a=1$: all input frequencies $u>.5$ are mapped to even higher perceived frequencies so that~$f(u)>u$. \emph{(ii)}~anti-conformity copying with $a=-.5$, leading to an effect in the opposite direction.\label{fig:usmregularisation1}]{\includegraphics[width=\maxwidth]{figure/usmregularisation-1} }
\subfloat[Mean expected change to an agent's usage level~$x$ for different values of~$T$, assuming learning rate~$\lambda=1$.\label{fig:usmregularisation2}]{\includegraphics[width=\maxwidth]{figure/usmregularisation-2} }

}

\caption{The dynamics of frequency-dependent selection as implemented in the utterance selection model through Equation~\ref{eq:usmregularisation}.}\label{fig:usmregularisation}
\end{figure}



What is crucial to understand about the USM is that these mapping functions~$f(.)$ affect the selection dynamics only somewhat indirectly. First of all, according to the definitions above the choice of resolution parameter~$T$ constrains the points in the $[0,1]$ range at which~$f(.)$ is actually ever evaluated, namely only at the fractions which can be sampled from the underlying binomial distribution, i.e.~$\{\frac{n}{T}\;|\;n=0\dots T\}$.
%These intermediate plots~(call them~$g_T(u)$) are crucial in understanding the analytical solutions that can be derived for the trajectories for different $f(u)$. The mathematical appendix to~\citet{Blythe2012} lays out how we can derive these solutions: given a function of the general form $f(u)=u+\lambda g(u)$
%The mean change of speaker frequency in one interaction and the average overall trajectory is simply dependent on the average of the function $g(u)$ over the distribution of token productions for a given current population mean~$x$. Assuming that tokens are produced independently, this is a binomial distribution with~$n=T$ and~$p=x$.
Particularly at low values, $T$~can therefore have a drastic impact on the dynamics. To visualise the effect of this parameter, we can determine the \emph{typical change} to an agent's usage rate~$x$ by a specific selection function~$f(.)$ for different values of~$x, T$. To do so we first calculate the mean perceived frequency~$\bar{y}$ over all possible sample outcomes~$n\sim Bin(T,x)$,
\begin{equation}\label{eq:meanperceivedfrequency}
\bar{y} = \sum_{n=0}^T P(n;T,x)\cdot f(\frac{n}{T})\;.
\end{equation}

Using Equation~\ref{eq:usmdiff} we can then determine the average change to an agent's usage level for some constant learning rate, which is plotted in Figure~\ref{fig:usmregularisation2}. The first striking observation is that, for~$T=2$, the mean expected change~$\Delta x$ is~0 for the entire range of values of~$x$, i.e.~the model is equivalent to the random copying of the neutral evolution condition. This result can be explained by inspecting the mapping functions directly above: with~$T=2$, the functions are only ever evaluated at $0, 0.5$ and $1$, all values for which they are identical to neutral copying, i.e.~where~$f(u)=u$. For higher values of~$T$, however, both regimes exhibit the expected influence on the agent's usage rate, with conformity-copying~(left panel) decreasing the usage of infrequent variants while further boosting the frequency of those which are already used more than $50\%$~of the time, and the opposite for anti-confirmity copying~(right panel). Generally, the higher~$T$, the stronger the impact of the function~$f(.)$, since bigger samples allow more evidence for variation\footnote{In particular, less probability mass on the two homogeneous samples $n=0$ and~$n=T$ to which $f(.)$ does not apply.}, a necessary ingredient for differential replication.

\citet{Blythe2012} report that with values of~$a>0$ any minority variants are rapidly eliminated from the population leading to the fixation of just one variant, while values~$a<0$ give rise to stable co-existence of all different variants, with the stability of co-existence dependent on the community size and learning rate of the individual agents~(p.286). These two regimes show that a frequency-dependent selection mechanism of this kind can be used to implement a bias for \emph{regularisation} (as well as~\emph{de-regularisation}) of competing linguistic variants. \index{regularisation}
However, neither of these two scenarios lead to directed transitions from the introduction of a novel variant to its complete adoption, as is the case in language change. While we will revisit the mechanism of regularisation based on \emph{innovation} rather than selection in a different evolutionary model in Section~\ref{sec:realigriffiths}, we now turn to the dynamics of USM configurations that implement \emph{asymmetries} of some kind.

\subsection{Weighted interactor selection}\index{interactor selection}\label{sec:usminteractorselection}\index{asymmetry}

Rather than rely on asymmetries in the \emph{replicators}~(the linguistic variants) that is implied by language-internal and variant prestige accounts, the more \emph{mechanical} social accounts discussed in Section~\ref{sec:interactorselection} seek the cause for the preferential spread of linguistic innovations in features of the social networks, such as differential interaction densities or the skewed influence of specific individuals or nodes in the network.
As discussed above, at least under the assumption of pure random copying the social network structure alone is not sufficient to alter the dynamics of neutral evolution to yield reliable directed transitions.
The idea of differential \emph{individual prestige} or influence however corresponds to a wholly separate mechanism, namely that of \emph{interactor selection}, where the asymmetry leading to the differential replication of variants is not due to the bias function~$f(.)$ but instead determined by the matrix~$H$ which controls the weight given to the samples obtained from different individuals in their interactions with others.

By adjusting the values in~$H$ accordingly, one can thus create an interaction structure where the production levels of some group of linguistically `leading' individuals is preferentially imitated by another group of `followers', but not vice versa. Proposals of this kind have been suggested to influence the diffusion of language changes through different \emph{adopter groups} of varying complexity~\citep{Labov2001,Rogers1962,Milroy1985}.
\citet{Blythe2012} use the utterance selection model to investigate the quantitative predictions made by different assumptions regarding the structure of adopter groups. Initiating only the `leading' group with high usage rates of an otherwise not established variant, they find that only a highly unrealistic staging of an entire chain of adopter groups, with the respective size of the groups following an exponential pattern, lead to s-shaped transitions. Having exhausted all other mechanisms of differential replication, they finally turn to the most direct way in which to affect the model dynamics, namely through a direct asymmetry between the linguistic replicators.

\subsection{Replicator selection}\index{replicator selection}\index{asymmetry}\label{sec:usmreplicator}

The most straightforward way to achieve a directed increase of one variant at the expense of the other is by implementing a bias function which consistently boosts the perceived frequency of that variant at the expense of all others. In this way, ``$f(u)>u$ for all frequencies $u$ between zero and one, and hence the listener perceives the innovation to be at a higher frequency than it was actually produced at, and overproduces accordingly''~\citep[p.291]{Blythe2012}. The simplest asymmetric linear function used by \citeauthor{Blythe2012},

\begin{equation}\label{eq:usmmultiplicative}
f(u) = u\cdot(1+b_m)\;,
\end{equation}
with a selection bias applied for all $b_m>0$, has an interesting property, namely that it is asymmetric in two ways: not only does it skew the likelihood of adoption towards one of the two competing variants, the strength with which this bias is applied also increases for higher frequencies of the innovative variant, as can be seen in Figure~\ref{fig:replicatorselection1}(i).
To confirm their finding that ``an S-shape is easily obtained through replicator selection''~(p.291) with little sensitivity to the precise selection function used, I will also be investigating a second model of replicator selection that employs an \emph{additive} instead of a multiplicative bias which exhibits equal strength across the entire trajectory,~i.e.

\begin{equation}\label{eq:usmadditive}
f(u)=u+b_a\;,
\end{equation}
again capped at the maximum value of 1.0, as can be seen in Figure~\ref{fig:replicatorselection1}(ii).
%To test whether this incremental amplification of the bias is necessary to produce s-shaped curves, I modified the replicator selection model to apply
In order to achieve a comparable bias strength for the two types of replicator selection, for all later figures the respective bias values  will satisfy $b_a=\frac{b_m}{2}$, a choice which results in equivalent strength selection at the mid-point as well as a (roughly) equal amplification relative to the neutral evolution condition~$f(u)=u$ across the entire trajectory.

From these bias functions~$f(.)$ we can again derive the average perceived frequency for different settings of~$T$ as well as the consequent expected change to the agents' usage frequencies, which are shown in Figure~\ref{fig:replicatorselection2}. The asymmetry of replicator selection is immediately evident from the fact that the expected change is always greater than zero, meaning that the incoming variant is boosted across the entire frequency range.
By solving the differential equations defined by these functions we can also calculate the average trajectories that would be produced as the selected for variant spreads through the population, which are shown in Figure~\ref{fig:replicatorselection3}~\citep[for an in-depth explanation of the approach see the appendix to][mathematical derivations of the results for both models of replicator selection are provided in Appendix~\ref{app:usm}]{Blythe2012}.

While for both models of selection the average changes to~$x$ at~$T=2$ result in growth patterns that are equivalent to the canonical s-shaped logistic function, \index{logistic growth}
the relative intensity of selection across the trajectory diverges for higher values of~$T$. For the original, multiplicative bias model shown in Figure~\ref{fig:replicatorselection3}(i), increasing the sample resolution leads to an extended period of initial growth in which the amount of change increments linearly past the half-way mark at which simple logistic growth characteristically starts to slow down again, making the growth pattern more and more exponential as the sample resolution~$T\rightarrow\infty$.
Consequently, this selection function predicts the maximum rate of growth, indicated by the peak in Figure~\ref{fig:replicatorselection2}(i), to occur later during the trajectory for higher settings of~$T$, while also exhibiting relatively greater levels of selection and thus faster transitions given the same bias strength~$b_m$.

A different picture emerges for the additive bias as shown in Figure~\ref{fig:replicatorselection2}(ii). While the overall selection pressure imposed by the bias function also increases for higher~$T$, the pattern of selection stays symmetric around the mid-point, with a relative acceleration of transitions achieved by a greater degree of selection in the other frequency regions.
For $T=2$ as well as $T=3$ the selection dynamics can be shown to be identical to logistic growth as displayed in Figure~\ref{fig:replicatorselection3}(ii), with growth rates of $2b_a$ and $3b_a$ respectively~(see Appendix~\ref{app:usm}). No general solution for the average expected trajectory is provided here but, since the function describing the mean change to~$x$ shown in Figure~\ref{fig:replicatorselection2}(ii) approaches a constant value throughout the interval~$(0,1)$ as $T$ increases, we should expect the dynamics to start resembling those of steady linear growth as~$T\rightarrow\infty$.

% Filling in the three different replicator biases and solving the differential equations for them we can thus deduce the average trajectories:

\begin{figure}[htbp]

{\centering \subfloat[Objective to perceived frequency mapping under replicator selection. \emph{(i)}~multiplicative replicator bias presented in~\citet{Blythe2012}, with $b_m=0.1$. \emph{(ii)} additive bias with equivalent bias strength~$b_a=b_m/2=0.05$. All mapping functions are capped at 0 and 1 and impose $f(0)=f(1)=0$. The dotted line indicates the unbiased mapping characteristic of neutral evolution, i.e.~$f(u)=u$.\label{fig:replicatorselection1}]{\includegraphics[width=\maxwidth]{figure/replicatorselection-1} }
\subfloat[Average bias applied with multiplicative and additive replicator selection for different values of~$T$, assuming~$\lambda=1$. The asymmetry of bias functions leads to an increase of the innovative variant across all frequency ranges, with generally stronger selection for higher~$T$. Mathematical derivations for the functions can be found in Appendix~\ref{app:usm}.\label{fig:replicatorselection2}]{\includegraphics[width=\maxwidth]{figure/replicatorselection-2} }
\subfloat[Typical average trajectories resulting from applying the multiplicative and additive replicator biases. For higher~$T$ the multiplicative bias extends the initial period of exponential growth, while the additive bias remains symmetric around the mid-point with fast growth from the very start of the trajectory.\label{fig:replicatorselection3}]{\includegraphics[width=\maxwidth]{figure/replicatorselection-3} }

}

\caption[Analysis of the selection dynamics for the original, multiplicative (left column) as well as additive (right column) replicator selection bias]{Analysis of the selection dynamics for the original, multiplicative (left column) as well as additive (right column) replicator selection bias.}\label{fig:replicatorselection}
\end{figure}



With this analysis we come to the end of an overview of some of the most relevant results that have been obtained from the computational version of the Utterance Selection Model originally due to~\citet{Baxter2006}.
Starting from an investigation of its baseline learning dynamics, which were shown to implement a model of \emph{neutral evolution}, we recapitulated the survey of pressures provided in \citet{Blythe2012} to determine which evolutionary replication mechanisms could lead to directed, s-shaped trajectories. \citeauthor{Blythe2012} concluded that only an inherent asymmetry between variants, implemented as a preference for the innovative variant that is shared by the majority of interacting agents, can reliably produce directed transitions. This conclusion can help us rule out some of the proposed pressures which, while not completely ineffective, do not appear to have the necessary leverage to be the main driving force behind the adoption of individual language changes, at least as far as one accepts the model's underlying assumptions regarding the behaviour of individual agents and their interactions.
% in particular those based on social network effects as well as individual prestige.
But, curiously, the result does not speak to the two biggest contenders to explaining language change: both language-internal and social, \emph{variant prestige} accounts are based on an underlying preference for an innovative over an outgoing variant, as implemented by a replicator selection bias. While this last Section showed how slightly different assumptions regarding the strength of the asymmetry and force of the selection pressure across different attested frequencies of the variants can yield slightly different predictions regarding the resulting trajectories, a look at the quantitative investigations of language changes presented in Chapter~\ref{ch:review} indicated that the empirical data might be too sparse to make any strong claims or perform conclusive comparisons between models and data~\citep[although see][]{Altmann2013,Ghanbarnejad2014}.

Despite the fact that the USM features a larger number of parameters than most other models of language evolution, every single one is both transparent and grounded in the USM's dedication to a concrete evolutionary framework~\citep{Croft2000}.
The sample resolution parameter~$T$, for example, while unusual in terms of its absence from other models of language change, is well-motivated by the model's strict separation of innovation and selection pressures.
The persistence with which the model has been analysed, both via analytical methods and numerical simulation, means that the sensibility of its dynamics~(or lack thereof) with regard to different parameters and parameter combinations are much better understood than for most other models~\citep{Blythe2007divided,Baxter2009,Blythe2009,Blythe2012neutral,Baxter2016,Michaud2017}.

Before I go on to expand the studies of the USM through another selection mechanism intended to tackle the question of how asymmetries between variants might emerge out of the replication dynamics itself, the main contribution of Chapter~\ref{ch:momentummodel}, I will first introduce a simpler, yet related, modelling framework that has also been used to investigate s-shaped curves, and which will also resurface again in Chapter~\ref{ch:conclusion}.

%Similarly, since $f(u)$ can never go above 1, the impact of any asymmetric selection functions are necessarily diminished when more and more datapoints that come in are at or close to $1$, which happens more often the closer the population average is to $1$. Thus, the current population average value of variant use also affects how exactly~$f(u)$ affects learning. This interaction is shown in the next Figure. 
