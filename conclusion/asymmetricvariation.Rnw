\section[Momentum with asymmetric generation of variants]{A simple momentum model with asymmetric generation of variants}
\label{sec:asymmetricvariation}

%One argument put forward in the previous sections was that, in order to explain asymmetries, it is not necessary to construe the prevalent conventions as having been selected for for that purpose.

In this section I will present a simple model to investigate how a symmetric momentum bias, a selection bias favouring trending conventions whether they are beneficial or deterious, interacts with asymmetric \emph{generation} of variants. The model is an extension of the Markov chain model of Bayesian Iterated Learning that was analysed in-depth in Section~\ref{sec:realigriffiths}.

To recapitulate, \citet{Reali2009} proposed a model of regularisation by Iterated Learning. In their model, a Bayesian learner infers the underlying relative frequencies of several competing variants based on a sample of productions they observe from a teacher. By iteratively passing one learner's output on to the next and analysing the \emph{stationary distribution} of this Markov chain, they showed that by setting the parameter~$\alpha$ to values that lead individual learners to slightly increase the variability of their own productions, the chains end up \emph{regularising} the input distribution: over time, the population ends up mostly producing input distributions where one of the competing variables is used categorically.

Moreover, in~\citet{Reali2010} the authors showed that under some circumstances this model of Bayesian Iterated Learning is equivalent to the Wright-Fisher model of biological evolution with mutation~\citep[see e.g.][]{Hartl2007}.
The Wright-Fisher model is a general tool from population genetics which is used to predict the expected change in the frequency of competing alleles in a population given specific \emph{mutation} (and in its extended form also \emph{selection}) probabilities.

\subsection{The Wright-Fisher model}
%While originally designed to describe the dynamics of biological evolution in a sexually reproducing population, the model is very general
In its very simplest form the Wright-Fisher model~\citep[for its original formulation see][]{Wright1931} describes the dynamics of competition between two variants in a finite population of size $N$.
Call the two variants 0 and 1 and their respective absolute frequencies $n_0$ and $n_1$. The entire population of $N$ individuals is replaced at discrete time steps so that it is always the case that $n_0 + n_1 = N$. To simplify notation and assuming a constant population size~$N$, we can again drop the indices referring to individual variants: we will henceforth write $n$ to mean $n_1$, from which the frequency of the competing variant can be trivially computed.

The state of the population at time~$t$ can be described simply by the relative frequency of that variant,~i.e.
\begin{equation}
x_t = \frac{n_t}{N}\;.
\end{equation}

The Wright-Fisher model assumes that the individual generations of the population as it evolves over time are \emph{non-overlapping}. In other words, the generation makeup at the following timestep, $n_{t+1}$, is determined by creating a new population of $N$ tokens, each of which the replication of a randomly selected `ancestor' from the previous generation.
The probability distribution over the likely frequencies $n_{t+1}$ in the next generation is consequently distributed according to a Binomial distribution

\begin{equation}
\label{eq:binomialsampling}
n_{t+1} \sim Bin(N, f(x_t))\;.
\end{equation}

In the absence of any other pressures, the probability of replicating an instance of a particular variant is simply equivalent to the relative frequency of that variant in the previous generation, i.e.~$f(x_t)=x_t$. The behaviour of this simplest form of the Wright-Fisher model describes the dynamics of completely neutral drift which was not only fundamental in informing the \emph{neutral theory} of genetic evolution~\citep{Kimura1968,Kimura1983}, but still forms an important baseline for evaluating the presence or absence of evolutionary pressures in empirical data sets to this day. % TODO reference?

Without any mechanisms to preferentially select any of the variants, or mutations that would introduce new variants, this is a very simple model of the \emph{diffusion} of traits through replication. Before we turn to more complex versions of the Wright-Fisher model which incorporate the influence of mutation and selection pressures, it is insightful to have a closer look at the similarity between diffusion models from biology and the Utterance Selection Model as described in Section~\ref{sec:usm}.\index{diffusion}

\subsubsection{Relationship to the Utterance Selection Model}

The similarity between Equation~\ref{eq:binomialsampling} and the data production function of the Utterance Selection Model in Equation~\ref{eq:usmsampling} is not just superficial: in its most general formulation the Wright-Fisher model is in fact identical to the trivial case of a single USM agent who is soliloquising with learning rate~$\alpha=1$ and USM sample resolution $T=N$.

More fully-fledged versions of the USM with populations of speakers also find parallels in more complex models of biological evolution. \citet{Blythe2007divided} showed how the learning and alignment dynamics of the USM are in fact identical to Wright's \emph{island} model, an extension of the simple Wright-Fisher model above used to study the diffusion of variants between subdivided populations with limited migration between them~\citep{Wright1931}.

While diffusion models are interesting in themselves and many results concerning fixation times can be derived from them analytically~\citep[see e.g.]{Baxter2008,Blythe2012copying}, we shall return to the simpler model of just one population with fixed turnover, which simplifies the analysis of different mutation and selection pressures that we are interested in here.

%While the diffusion model interesting in itself as a baseline or \emph{null model} of change, we are currently more interested in the different ways in which \emph{biases} or \emph{pressures} can act on a population of replicating units.

\subsubsection{The Wright-Fisher model with mutation}\index{mutation}
\label{sec:realigriffithsequivalence}

% best reference it seems: http://numerical.recipes/whp/notes/wrightfisher.pdf

Under the Wright-Fisher model with \emph{mutation}, the probability of producing an instance of variant~$0$ at the next generation is not completely equivalent to its current prevalence $x$. Instead, with probability~$\mu_{1}$, one of the $n_0$ type~0 variants present in the population will spontaneously mutate into an instance of variant~1 during its replication. Conversely, there is a probability of~$\mu_{0}$ that one of the instances of the other variant~(of which there are $n_1$) will spontaneously mutate into a variant~0. Under this assumption, the relative probability of producing an instant of variant~1 at the next generation is

\begin{equation}\label{eq:mutation}
\phi(x) = x\cdot(1-\mu_0) + (1-x)\cdot\mu_1 \;.
\end{equation}

It is this version of the Wright-Fisher model that is mathematically equivalent to \citeauthor{Reali2010}'s model of Bayesian inference, in particular to the version of the model where the learners derive a hypothesis from their input sample of size $N$ by deterministically selecting the mean~$\hat{\theta}$ of the posterior distribution~$p(\theta|x)$. In this case, the learner's production behaviour is identical to that of a population of~$N$ individuals who are replicating with mutation rates set to
\begin{equation}
\mu_0 = \mu_1 = \frac{\alpha}{2\cdot(\alpha+N)}\;.
\end{equation}

% according to both the regularisation parameter~$\alpha$ as well as the population size~$N$,

%The fact that equivalence can only be established as a function of both of the Bayesian model's parameters is slightly awkward, as it means that, to achieve the same regularisation behaviour~(by means of setting~$\alpha$) in a population, 

%This equivalence comes in the wake of other studies which have highlighted that evolution by natural selection can be seen as an inference dynamics~\citep{Harper2009}.

The Wright-Fisher model with mutation allows for pressures in favour of specific variants, by setting the mutation rates $\mu_1\ne\mu_0$.
%In particular, by setting only one of the rates to 0 one can simulate the accumulation of `noise'
But it does not support \emph{selection} of variants, as would be needed for a replicator or momentum selection bias.

\subsubsection{Wright-Fisher model with mutation and selection}

To allow selection on top of mutation, we have to move on to the Wright-Fisher model with mutation and selection, which takes the form

\begin{equation}\label{eq:mutationselection}
f(x) = \frac{x\cdot(1+s)\cdot(1-\mu_0) + (1-x)\cdot\mu_1}{x\cdot(1+s) + 1 - x} \;.
\end{equation}

The parameter $s\ge 0$ in this equation represents a \emph{selection coefficient} which causes the probability of the $n_1$~tokens of variant~$1$ that are present in the population to be preferentially selected, i.e. their likelihood of replication is increased at the expense of the competing variant. In contrast to the mutation pressures~$\mu$, the effectiveness of the selection coefficient depends on the current prevalence of the selected variant in the population. In particular, the selection pressure in favour of an advantageous variant is completely ineffective as long as no tokens of that variant are present in the population. In other words, the selection coefficient cannot introduce new variants. Also, selection acts strongest when there is most variation, i.e.~when both variants are equally represented in the population.

The precise impact of the different parameters will become more evident as we study the dynamics caused by applying the different pressures. To investigate the dynamics of these different types of Wright-Fisher models, we can again turn to the Markov model framework introduced in Section~\ref{sec:markovmodel} as a tool for analysis.

\subsection[Modelling the interaction of different pressures]{Modelling the interaction of different pressures with the Wright-Fisher model}

\subsubsection{Diffusion without innovation or selection}

With pure diffusion, i.e.~in the absence of any innovation of new variants, both categorical states are \emph{absorbing} states. The exact probability of ending up in either absorbing state depends on the initial condition, as can be seen in Figure~\ref{fig:symmetricmutation1}.

<<symmetricmutation, echo=FALSE, fig.cap=paste("Stationary distribution of the Wright-Fisher model.", fig.subcap=c("Pure diffusion, i.e.~$\\mu_0=\\mu_1=0$. Both `pure' states are absorbing states, the probability of the population ending up in either depends on the initial frequency of the two variants: ", "Symmetric mutation, i.e.~$\\mu_0=\\mu_1>0$. The shape of the stationary distribution depends on the mutation rate and population size $N$.">>=
source("../knitr-setup.R")
source("../R/markovchain.R")
#source("../R/hmm.R")

N <- 50
mus <- c(.005, .01, .1)

tightmargin(mfrow=c(1, 3))
# TODO 3 different initstates - do manual markov chain?
plotstationary(binomialsampling.markov.matrix(repl.mut.eq(N=N)))
plotstationary(binomialsampling.markov.matrix(repl.mut.eq(N=N)))
plotstationary(binomialsampling.markov.matrix(repl.mut.eq(N=N)))

tightmargin(mfrow=c(1, 3))
for (mu in mus)
  plotstationary(binomialsampling.markov.matrix(repl.mut.eq(N=N, m0=mu, m1=mu)), main=bquote(mu ~ "=" ~ .(mu)))
@

\subsubsection{Symmetric mutation}

The case of symmetric mutation, where the probabilities of spontaneously inventing either of the competing variants are equal~($\mu_0 = \mu_1$) has been shown to be equivalent to \citeauthor{Reali2010}'s Bayesian Iterated Learning chain of \emph{averaging} learners. For a quantitative analysis of the dynamics under such settings, see Section~\ref{sec:realigriffiths}. To recapitulate, under moderately low mutation rates the stationary distribution is similar to the one of the pure diffusion model, only that the non-zero probability of randomly producing an unattested variant means that variation is never fully eliminated from the system. Depending on the mutation rate $\mu$ and the population size $N$ as well as their relationship. Under small mutation rates, the stationary distribution will either primarily exhibit states with no variation, whereas high mutation rates will lead the populations to mostly consist of an even mix of all variants. A summary of the different regimes of the stationary distribution is shown in Figure~\ref{fig:symmetricmutation2}.

\subsubsection{Asymmetric mutation as a model of the accumulation of errors}

As mentioned before, one typically speaks of a \emph{bias} when one variant is treated differently from the other on some level, i.e.~when there is some \emph{asymmetry} between variants. The simplest way to introduce asymmetry into the Wright-Fisher model is by setting the two variants' mutation or innovation probabilities unequal, i.e.~$\mu_0\ne\mu_1$.

Such a scenario maps closely onto the earliest theories of language change discussed in Chapter~\ref{ch:review}: assuming that a linguistic variant `comes out as another' some of the time, for example due to coarticulation effects, maps neatly onto a scenario with moderately low mutation rates, where one vastly outweights the other one. % ~(in the most extreme case by setting one of them to~0)
In other words, the Wright-Fisher model with mutation lends itself as a model of change through the gradual accumulation of errors.

Figure~\ref{fig:asymmetricmutation} shows the Markov chain analysis of that there thing. TODO

<<asymmetricmutation, fig.cap=paste("Dynamics of the Wright-Fisher model with strong mutation in one direction, $N=", N, ", \\mu_1=", m1, ", \\mu_0=", m0, "$.", sep=""), fig.subcap=c("Development of the chain", "Stationary distribution")>>=
source("../R/hmm.R")
Ns <- c(20, 50, 100)
m1 <- mus[1]
m0 <- m1/100
ngenerations <- 200

tightmargin(mfrow=c(1, 3), pty="s")
for (N in Ns)
  repl.mut.eq(N=N, m0=m0, m1=m1) %>%
  binomialsampling.markov.matrix %>%
  markov.chain(., ngenerations) %>%
  plotchain

tightmargin(mfrow=c(1, 3), pty="s")
for (N in Ns)
  repl.mut.eq(N=N, m0=m0, m1=m1) %>%
  binomialsampling.markov.matrix %>%
  plotstationary(., ylim=0:1)

#plotcompletionprobabilities( # exactduration=TRUE
@

\subsubsection{The dynamics of selection}\index{selection,replicator selection}

<<selection>>=
@

The presence of a selection bias for individual variants, equivalent to a \emph{replicator selection},
We can reproduce the same (or at least similar) dynamics as for the Utterance Selection Model~(see Section~\ref{sec:usm}).

\subsection{A Markov chain state space for momentum}

The simple models above recapitulate the criticisms presented in Section~\ref{sec:review}: accumulation of error and selection predict~(deterministic) selection only in one way, with no chance of returning.

Interaction between them does what?

On the face of it, the idea of momentum and the assumptions of a Markov model discussed in Section~\ref{sec:markov} seem at odds: the Markov assumption states explicitly that 

While the Markov model itself is oblivious to the structure of its state space, a meaningful state space can be constructed. In order to represent a population of $N$ individuals (or memory size of $N$ tokens), \citeauthor{Reali2009} constructed a space of $N+1$ states. On some level, this state space was unstructured: all states had non-zero (if very small) transition probabilities to each other, forming one fully connected state space. But on top of this homogeneous structure there was semantically meaningful organisation of states: each state corresponded to a certain memory state of an agent, with corresponding transition probabilities into the next states~(see Figure~\ref{fig:markovstatespacesimple}).

In order to augment this model with a momentum bias, we simply have to duplicate the number of states: for every state of the \citeauthor{Reali2009} model which represents a certain prevalence $x$ of variant~$0$, we create two additional states, representing the same value of $x$, but with positive and negative momentum terms~$m$.

A schematic visualisation of the shape of this state space is shown in Figure~\ref{fig:markovstatespacemomentum}.

\begin{figure}

\newcommand{\N}{5}
\newcommand{\Nm}{4}
\newcommand{\Nmm}{3}

\begin{subfigure}[b]{\textwidth}
\centering
\begin{tikzpicture}[scale=1.7]
\tikzset{every state/.style={minimum size=1em}}
\node at (-1,-1) {x =};
\node[text=white] at (-2,-1) {m = -1}; % for identical graph width with below

\foreach \x in {0,...,\N} {
  \node at (\x,-1) {$\x$};
  \node[state] (\x) at (\x,0) {};
  \path[->] (\x) edge  [loop left] node {} ();
}

\foreach \x in {0,...,\Nm} {
  \foreach \y in {\N,...,\number\numexpr\x+1}
    \pgfmathtruncatemacro{\diff}{100*(\y-\x)/\N}
    \path[->,draw=white!\diff!black]
      (\x) edge [bend left] node {} (\y)
      (\y) edge [bend left] node {} (\x);
}
\end{tikzpicture}
\caption{State space of the~\citet{Reali2009} Markov model with $N=\N$.}\label{fig:markovstatespacesimple}
\end{subfigure}

\begin{subfigure}[b]{\textwidth}
\centering
\begin{tikzpicture}[scale=1.7]
\tikzset{every state/.style={fill=white,minimum size=1em}}

\node at (-1,-2) {x =};
\node at (-2,-1) {m = -1};
\node at (-2,0) {m = 0};
\node at (-2,1) {m = 1};

% momentum states
\foreach \x in {0,...,\Nm} {
  \pgfmathtruncatemacro{\next}{\x + 1}
  \node[state] (\x-1) at (\x,-1) {};
  \node[state] (\next+1) at (\next,1) {};
  \pgfmathtruncatemacro{\diff}{100/\N}
  \path[<->,draw=white!\diff!black] % immediate reversal
    (\x-1) edge node {} (\next+1); % [bend left=15] to avoid crossing loops
}

% paths along momentum states
\foreach \x in {\Nm,...,1} {
  \pgfmathtruncatemacro{\prev}{\x-1}
  \pgfmathtruncatemacro{\next}{\x+1}

%  \path[->] (\x-1) edge node {} (\prev-1)
%            (\x+1) edge node {} (\next+1);

% along top to right
%  \ifnum \prev>0, draw from \number\numexpr\prev-1\relax only
    \foreach \y in {\prev,...,0} {
      \pgfmathtruncatemacro{\diff}{100*(\x-\y-1)/\N}
      \path[->,draw=white!\diff!black]
        (\x-1) edge [bend left] node {} (\y-1);
    }
%  \fi
% along bottom to left
  \foreach \y in {\N,...,\next} {
    \pgfmathtruncatemacro{\diff}{100*(\y-\x-1)/\N}
    \path[->,draw=white!\diff!black]
      (\x+1) edge [bend left] node {} (\y+1);
  }
}

% long distance reversals
\foreach \x in {0,...,\Nmm} {
  \pgfmathtruncatemacro{\nnext}{\x + 2}
  \foreach \y in {\nnext,...,\N} {
    \pgfmathtruncatemacro{\diff}{100*(\y-\x)/(\N+1)}
    \path[<->,draw=white!\diff!black] (\x-1) edge node {} (\y+1);
    % [bend left=15]
  }
}

% momentum-free states
\foreach \x in {0,...,\N} {
  \node at (\x,-2) {$\x$};
  \node[state] (\x0) at (\x,0) {};
}

% straight paths out of and into momentum-free states
\foreach \x in {0,...,\Nm} {
  \pgfmathtruncatemacro{\next}{\x+1}
  % initiate increase (longest jumps first)
  \foreach \y in {\N,...,\next} {
    \pgfmathtruncatemacro{\diff}{100*(\y-\x-1)/\N}
    \path[->,draw=white!\diff!black] (\x0) edge node {} (\y+1);
  }
  % initiate decrease
  \foreach \y in {0,...,\x} {
    \pgfmathtruncatemacro{\diff}{100*(\x-\y)/\N}
    \path[->,draw=white!\diff!black] (\next0) edge node {} (\y-1);
  }
  % stalling
  \pgfmathtruncatemacro{\diff}{200/\N}
  \path[->,draw=white!\diff!black]
    (\x-1) edge node {} (\x0)
    (\next+1) edge node {} (\next0);
}
% old: curved initation straight to goal
%\pgfmathtruncatemacro{\diff}{100*(\N-1)/\N}
%\path[->,draw=white!\diff!black]
%  (00) edge [bend left=53] node {} (\N+1)
%  (\N0) edge [bend left=53] node {} (0-1);


% completion (drawn in gray right above by default)
%\path[->]
%  (0-1) edge node {} (00)
%  (\N+1) edge node {} (\N0);

% loops
\path[->,out=205,in=155] \foreach \x in {0,...,\N}
  {(\x0) edge [loop] node {} ()};


\end{tikzpicture}

\caption{State space of the Markov model with $N=\N$ and a momentum bias $b>0$.}\label{fig:markovstatespacemomentum}
\end{subfigure}

\caption[Schematic visualisation of the Markov model state space]{Schematic visualisation of the Markov model state space. Colouring of the edges indicates relative probability of the transitions, with darker edges representing more likely transitions.} \label{fig:markovstatespace}
\end{figure}

While the number of states of this momentum model is almost threefold in comparison to the baseline model, the pattern of transitions is actually not much more complex. In particular, the model is not fully connected: every state has exactly $N$ outward transitions, exactly one each to every level of $x=0\ldots N$. 
The semantics of these three parallel states determines which of them a given previous state will transition into: all transitions from states with a lower $x$ go into the $m=1$ state, transitions from higher values of $x$ go into the $m=-1$ state, and only transitions from identical $x$ values enter the state with~$m=0$.

In order to affect the dynamics of the system, probabilities of transitioning between different levels of $x$ are affected by the value of the momentum term: for the middle row with $m=0$, the probabilities of producing a given~$x$ are equivalent to the \citeauthor{Reali2009} model, which means they correspond to the Wright-Fisher model with mutation only, as in Equation~\ref{eq:mutation}.

For the upper and lower rows in the diagram, corresponding to a positive~(top)
 or negative~(bottom) momentum term, the momentum affects the probabilities of producing a certain number of $x$~tokens by exerting a selection pressure on the variant that is currently `trending', i.e. whose frequency $x$ has increased at the last timestep. To this end the transition probabilities out of a state with $m=1$ are calculated according to the Wright-Fisher model with mutation and selection as given in Equation~\ref{eq:mutationselection}, with the selection coefficient $s$ in favour of variant $0$ set to a fixed constant. Conversely, the transition probabilities for states with $m=-1$ are controlled by the same equation, only that the same selection coefficient is selecting variant $1$. Note how, as long as the mutation probabilities $\mu_0, \mu_1$ are equal, this system is again \emph{symmetric}. Even though there are clear paths of directed selection~(e.g. along ) these paths are mirrored on the other side.

A concrete example of a Markov chain transition matrix for such a momentum model with $N=5, \alpha=?, b=$ is given in Table~\ref{tbl:momentumtransitionmatrix}.

<<momentum>>=

momentum.matrix.pos <- function(sourcemomentum, targetmomentum, maxmomentum=1) {
  Nm <- 1+2*maxmomentum
  N <- max(nrow(sourcemomentum), nrow(targetmomentum))
  # column (target) offsets
  col <- matrix(rep(maxmomentum+Nm*0:(N-1), each=N), nrow=N) + targetmomentum
  Nm*N*col + rep(1+maxmomentum+Nm*0:(N-1), N)+sourcemomentum
}

# 'up/down' momentum, i.e. momentum term always one of -1, 0, 1
updown.momentum.matrix <- function(N, ..., b=0) {
  # unbiased (binomial) sampling a la Wright-Fisher
  tm <- repl.mut.matrix(N, ...)
  m <- matrix(0, nrow=3*nrow(tm), ncol=3*nrow(tm))

  targetmomentums <- upper.tri(tm)-lower.tri(tm)
  # first take over all uninfluenced production probabilities
  m[momentum.matrix.pos(0, targetmomentums)] <- tm

  # calculate momentum-influenced production probabilities and add them
  for (n in 1:nrow(tm)) {
    posmtargets <- momentum.matrix.pos(1, targetmomentums)
    m[posmtargets] <- binomialsampling.markov.matrix(repl.mut.eq(N, ..., b=b))
    negmtargets <- momentum.matrix.pos(-1, targetmomentums)
    m[negmtargets] <- binomialsampling.markov.matrix(repl.mut.eq(N, ..., b=-b))
  }
  newchain(m, name=paste("Binary (up/down) momentum with b", b, sep="="), states=as.vector(t(outer(paste("x", 0:N, sep="="), paste("m", c(-1, 0, 1), sep="="), paste))))
}
#updown.momentum.matrix(3, b=1)

alpha <- .1
b <- .3

latextable(updown.momentum.matrix(3, alpha0=alpha, b=b)[], caption="Markov chain transition matrix with momentum.", label="tbl:momentumtransitionmatrix")
@

How do the dynamics of this momentum selection model differ from the original, mutation-only model? A direct comparison of the two models' stationary distributions for various settings of the regularisation parameter~$\alpha$ is shown in Figure~\ref{fig:momentumstationary}.

First of all, we can test whether the model is actually equivalent by setting the momentum bias to $b=0$~(Figure~\ref{fig:momentumstationary1})
Despite the bias being ineffective in this model, the colour indication of the momentum term is informative: it shows how much of the time the model \\emph{remains} at a given proportion, resulting in a momentum term of $0$ (indicated in white).

<<momentumstationary, fig.cap="Stationary distributions of Markov chains with momentum (various $\\alpha, b$).", fig.subcap=c("Without a momentum bias~($b=0$) the stationary distribution is identical to \\citet{Reali2009}'s \\emph{averaging} learner~(compare Figure~\\ref{fig:averagerstationarydistribution}).", paste("With a momentum bias of $b=", bs[2], "$, the model naturally avoids probability matching as the model preferentially sweeps through this middle region in a directed fashion. The time spent in momentum-free states in the middle reduces. TODO Explain red=$1$, blue$=-1$.", sep=""), paste("With an even higher momentum bias $b=", bs[3], "$, the model spends almost no time in the middle region.", sep=""))>>=

plotmomentumstationary <- function(markovchain, maxmomentum=0, symmetric=TRUE, col=c("blue", "white", "red"), xlab="x", ...) { # TODO space default?
  if (symmetric) {
    # symmetric colouring: plot data with left half zeroed first
    st <- matrix(steadyStates(markovchain), nrow=1+2*maxmomentum)
    halfway <- ceiling(dim(st)[2]/2)
    barplot(cbind(matrix(0, nrow=3, ncol=halfway), st[dim(st)[1]:1,(halfway+1):(dim(st)[2])]), col=rev(col), names.arg=0:(dim(st)[2]-1), xlab=xlab, ...)
    barplot(st[,1:halfway], col=col, add=TRUE)
  } else { # asymmetric (+-momentum doesn't flip at mid-point)
    plotstationary(st=matrix(steadyStates(markovchain), nrow=1+2*maxmomentum), col=col, ...)
#    plotstationary(st=t(colSums(matrix(steadyStates(markovchain), nrow=1+2*maxmomentum))), ...)
  }
}
#par(mfrow=1:2)
#plotmomentumstationary(updown.momentum.matrix(10, alpha0=1), 1, symmetric=FALSE)
#plotmomentumstationary(updown.momentum.matrix(10, alpha0=1), 1, symmetric=TRUE)

alphas <- c(0.5, 1, 2)
N <- 10
bs <- c(0, 1, 5)

for (b in bs) {
  tightmargin(mfrow=c(1,3), pty="s")
  for (alpha in alphas)
    plotmomentumstationary(updown.momentum.matrix(N, alpha0=alpha, b=b), 1, main=formatalpha(alpha), ylim=c(0, 0.25))
}

# stationary with b=0 is actually identical to the averaging one
#for (alpha in c(0.2, 1, 2)) {
#  plotstationary(bilm.transition.matrix.average(N, alpha))
#  plotmomentumstationary(updown.momentum.matrix(N, alpha0=alpha), 1)
#}
@