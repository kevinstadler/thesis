\section[Momentum with asymmetric generation of variants]{A simple momentum model with asymmetric generation of variants}
\label{sec:asymmetricvariation}

%One argument put forward in the previous sections was that, in order to explain asymmetries, it is not necessary to construe the prevalent conventions as having been selected for for that purpose.

In this section I will present a simple model to investigate how a symmetric momentum bias, a selection bias favouring trending conventions whether they are beneficial or deterious, interacts with asymmetric \emph{generation} of variants. The model is an extension of the Markov chain model of Bayesian Iterated Learning that was analysed in-depth in Section~\ref{sec:realigriffiths}.

To recapitulate, \citet{Reali2009} proposed a model of regularisation by Iterated Learning. In their model, a Bayesian learner infers the underlying relative frequencies of several competing variants based on a sample of productions they observe from a teacher. By iteratively passing one learner's output on to the next and analysing the \emph{stationary distribution} of this Markov chain, they showed that by setting the parameter~$\alpha$ to values that lead individual learners to slightly increase the variability of their own productions, the chains end up \emph{regularising} the input distribution: over time, the population ends up mostly producing input distributions where one of the competing variables is used categorically.

Moreover, in~\citet{Reali2010} the authors showed that under some circumstances this model of Bayesian Iterated Learning is equivalent to the Wright-Fisher model of biological evolution with mutation~\citep[see e.g.][]{Hartl2007}.
The Wright-Fisher model is a general tool from population genetics which is used to predict the expected change in the frequency of competing alleles in a population given specific \emph{mutation} (and in its extended form also \emph{selection}) probabilities.

\subsection{The Wright-Fisher model}
%While originally designed to describe the dynamics of biological evolution in a sexually reproducing population, the model is very general
In its very simplest form the Wright-Fisher model~\citep[for its original formulation see][]{Wright1931} describes the dynamics of competition between two variants in a finite population of size $N$.
Call the two variants 0 and 1 and their respective absolute frequencies $n_0$ and $n_1$. The entire population of $N$ individuals is replaced at discrete time steps so that it is always the case that $n_0 + n_1 = N$. To simplify notation and assuming a constant population size~$N$, we can again drop the indices referring to individual variants: we will henceforth write $n$ to mean $n_1$, from which the frequency of the competing variant can be trivially computed.

The state of the population at time~$t$ can be described simply by the relative frequency~$x$ of that variant,~i.e.
\begin{equation}
x_t = \frac{n_t}{N}\;.
\end{equation}

The Wright-Fisher model assumes that the individual generations of the population as it evolves over time are \emph{non-overlapping}. In other words, the generation makeup at the following timestep, $n_{t+1}$, is determined by creating a new population of $N$ tokens, each generated by replicating a randomly selected `ancestor' from the previous generation.
The probability distribution over the likely frequencies $n_{t+1}$ in the next generation is consequently distributed according to a Binomial distribution

\begin{equation}
\label{eq:binomialsampling}
n_{t+1} \sim Bin(N, f(x_t))\;.
\end{equation}

In the absence of any other pressures, the probability of replicating an instance of a particular variant is simply equivalent to the relative frequency of that variant in the previous generation, i.e.~$f(x_t)=x_t$. The behaviour of this simplest form of the Wright-Fisher model describes the dynamics of completely neutral drift which was not only fundamental in informing the \emph{neutral theory} of genetic evolution~\citep{Kimura1968,Kimura1983}, but still forms an important baseline for evaluating the presence or absence of evolutionary pressures in empirical data sets to this day. % TODO reference?

Without any mechanisms that influence the selection of existing variants or mutations that would introduce new ones, this is a very simple model of the \emph{diffusion} of traits through replication. Before we turn to more complex versions of the Wright-Fisher model which incorporate the influence of mutation and selection pressures, it is insightful to have a closer look at the similarity between diffusion models from biology and the Utterance Selection Model as described in Section~\ref{sec:usm}.\index{diffusion}

\subsubsection{Relationship to the Utterance Selection Model}

The similarity between Equation~\ref{eq:binomialsampling} and the data production function of the Utterance Selection Model in Equation~\ref{eq:usmsampling} is not just superficial: in its most general formulation the Wright-Fisher model is in fact identical to the trivial case of a single USM agent with a learning rate~$\alpha=1$ and USM sample resolution $T=N$ that is engaged in a production-perception loop.\index{production-perception loop}

More fully-fledged versions of the USM with populations of speakers also find parallels in more complex models of biological evolution. \citet{Blythe2007divided} showed how the learning and alignment dynamics of the USM are in fact identical to Wright's \emph{island} model, an extension of the simple Wright-Fisher model above that is used to study the diffusion of variants between subdivided populations with limited migration between them~\citep{Wright1931}.

While diffusion models are interesting in themselves and many results concerning fixation times can be derived from them analytically~\citep[see e.g.][]{Baxter2008,Blythe2012copying}, we shall return to the simpler model of just one population with fixed turnover, which greatly simplifies the analysis of different mutation and selection pressures that we are interested in here.

%While the diffusion model interesting in itself as a baseline or \emph{null model} of change, we are currently more interested in the different ways in which \emph{biases} or \emph{pressures} can act on a population of replicating units.

\subsubsection{The Wright-Fisher model with mutation}\index{mutation}
\label{sec:realigriffithsequivalence}

% best reference it seems: http://numerical.recipes/whp/notes/wrightfisher.pdf

Under the Wright-Fisher model with \emph{mutation}, the probability of producing an instance of variant~$0$ at the next generation is not completely equivalent to its current prevalence $x$. Instead, with probability~$\mu_{1}$, one of the $n_0$ type~0 variants present in the population will spontaneously mutate into an instance of variant~1 during its replication. Conversely, there is a probability of~$\mu_{0}$ that one of the instances of the other variant~(of which there are $n_1$) will spontaneously mutate into a variant~0. Under this assumption, the relative probability of producing an instant of variant~1 at the next generation is

\begin{equation}\label{eq:mutation}
f(x) = x\cdot(1-\mu_0) + (1-x)\cdot\mu_1 \;.
\end{equation}

It is this version of the Wright-Fisher model that is mathematically equivalent to \citeauthor{Reali2010}'s model of Bayesian inference, in particular to the version of the model where the learners derive a hypothesis from their input sample of size $N$ by deterministically adopting the mean~$\hat{\theta}$ of the posterior distribution~$p(\theta|x)$ as their production probability. In this case, the learner's production behaviour is identical to that of a population of~$N$ individuals who are replicating with mutation rates set to
\begin{equation}
\mu_0 = \mu_1 = \frac{\alpha}{2\cdot(\alpha+N)}\;.
\end{equation}

% according to both the regularisation parameter~$\alpha$ as well as the population size~$N$,

%The fact that equivalence can only be established as a function of both of the Bayesian model's parameters is slightly awkward, as it means that, to achieve the same regularisation behaviour~(by means of setting~$\alpha$) in a population, 

%This equivalence comes in the wake of other studies which have highlighted that evolution by natural selection can be seen as an inference dynamics~\citep{Harper2009}.

The Wright-Fisher model with mutation allows for pressures in favour of specific variants, by setting the mutation rates $\mu_1\ne\mu_0$.
%In particular, by setting only one of the rates to 0 one can simulate the accumulation of `noise'
But it does not support \emph{selection} of variants, as would be needed for a replicator or momentum-based selection bias.

\subsubsection{Wright-Fisher model with mutation and selection}

To allow selection on top of mutation, we have to move on to the Wright-Fisher model with mutation and selection, which takes the form

\begin{equation}\label{eq:mutationselection}
f(x) = \frac{x\cdot(1+s)\cdot(1-\mu_0) + (1-x)\cdot\mu_1}{x\cdot(1+s) + 1 - x} \;.
\end{equation}

The parameter $s\ge 0$ in this equation represents a \emph{selection coefficient} which causes the probability of the $n_1$~tokens of variant~$1$ that are present in the population to be preferentially selected, i.e. their likelihood of replication is increased at the expense of the competing variant. In contrast to the mutation pressures~$\mu$, the effectiveness of the selection coefficient depends on the current prevalence of the selected variant in the population. In particular, the selection pressure in favour of an advantageous variant is completely ineffective as long as no tokens of that variant are present in the population. In other words, the selection coefficient cannot introduce new variants. Also, selection acts strongest when there is most variation, i.e.~when both variants are equally represented in the population.

The precise impact of the different parameters will become more evident as we study the dynamics caused by applying the different pressures. To investigate the dynamics of these different types of Wright-Fisher models, we can again turn to the Markov model framework introduced in Section~\ref{sec:markovmodel} as a tool for analysis.

\subsection[Modelling the interaction of different pressures]{Modelling the interaction of different pressures with the Wright-Fisher model}

\subsubsection{Diffusion without innovation or selection}

With pure diffusion, i.e.~in the absence of any innovation of new variants, both categorical states are \emph{absorbing} states: starting off with a population exhibiting variation, continuous replication of a finite population will eventually lead all but one of the variants to die out. The exact probability of either variant diffusing across the entire population depends on the initial condition of the population, as can be seen in Figure~\ref{fig:symmetricmutation1}.

<<setup, echo=FALSE>>=
source("../knitr-setup.R")
source("../R/markovchain.R")
#source("../R/hmm.R")
@

<<symmetricmutation, fig.cap="Stationary distributions of the Wright-Fisher model under different mutation regimes.", fig.subcap=c("Pure diffusion, i.e.~$\\mu_0=\\mu_1=0$. Both `pure' states are absorbing states, the probability of the population ending up in either absorbing state depends on the initial frequency of the two variants: FIXME", "Symmetric mutation, i.e.~$\\mu_0=\\mu_1>0$. The shape of the stationary distribution depends on the mutation rate and population size $N$.")>>=
N <- 50
xs <- c(.1, .2, .5)
mus <- c(.005, .01, .1)

tightmargin(mfrow=c(1, 3), pty="s")
# TODO 3 different initstates - do manual markov chain?
for (x in xs)
  plotstationary(binomialsampling.markov.matrix(repl.mut.eq(N=N)), main=bquote(x[0] ~ "=" ~ .(x)))

tightmargin(mfrow=c(1, 3), pty="s")
for (mu in mus)
  plotstationary(binomialsampling.markov.matrix(repl.mut.eq(N=N, m0=mu, m1=mu)), main=bquote(mu ~ "=" ~ .(mu)))
@

\subsubsection{Symmetric mutation}

As mentioned previously the case of symmetric mutation, where the probabilities of spontaneously inventing either of the competing variants are equal~($\mu_0 = \mu_1$), has been shown to be equivalent to \citeauthor{Reali2010}'s Bayesian Iterated Learning chain of \emph{averaging} learners. An in-depth quantitative analysis of the dynamics under such settings was provided in Section~\ref{sec:realigriffiths}. To recapitulate, under moderately low mutation rates the stationary distribution is similar to the one of the pure diffusion model, only that the non-zero probability of randomly producing an unattested variant means that variation is never fully eliminated from the system. High mutation rates on the other hand will lead the populations to mostly consist of an even mix of all variants considered by the model, with the exact cutoff point between the behaviours depending both on the mutation rate~$\mu$ and the population size~$N$ as well as the relationship of the two~(see Section~\ref{sec:realigriffithsequivalence}). A summary of the different regimes of the stationary distribution is shown in Figure~\ref{fig:symmetricmutation2}.

\subsubsection{Asymmetric mutation as a model of the accumulation of errors}

As mentioned before, one typically speaks of a \emph{bias} when one variant is treated differently from the other on some level, i.e.~when there is some \emph{asymmetry} between variants. The simplest way to introduce asymmetry into the Wright-Fisher model is by setting the two variants' mutation or innovation probabilities unequal, i.e.~$\mu_0\ne\mu_1$.

Such a scenario maps closely onto some of the earliest theories of language change discussed in Chapter~\ref{ch:review}: the assumption that a linguistic variant is accidentally `misproduced' as another some of the time, for example due to coarticulation effects, maps neatly onto a scenario with moderately low mutation rates, where the innovation probability in one direction vastly outweighs the one in the other~(which might in fact be 0). % ~(in the most extreme case by setting one of them to~0)
In other words, the Wright-Fisher model with mutation lends itself as a model of change through the \emph{gradual accumulation of errors}.\index{accumulation of errors}

TODO Figure~\ref{fig:asymmetricmutation} shows the Markov chain analysis of that there thing: transitions in the direction of the more frequently invented variant look much different from those in the other, and even if a more complex state space (e.g. one with three competing variants and a unidirectional change cycle between them) was construed, changes in one direction actuate instantly and are not s-shaped.

<<asymmetricmutation, fig.cap=paste("Dynamics of the Wright-Fisher model with strong mutation in one direction ($\\mu_1=", mu[2], ", \\mu_0=", mu[1], "$) for three different population sizes~$N=", paste(Ns, collapse=", "), "$.", sep=""), fig.subcap=c("State probability distribution over time.", "Stationary distribution of the population.")>>=
source("../R/hmm.R")
Ns <- c(20, 50, 100)
mu <- c(mus[2]/10, mus[2])
ngenerations <- 200

tightmargin(mfrow=c(1, 3), pty="s")
for (N in Ns)
  repl.mut.eq(N=N, m0=mu[1], m1=mu[2]) %>%
  binomialsampling.markov.matrix %>%
  markov.chain(., ngenerations) %>%
  plotchain

tightmargin(mfrow=c(1, 3), pty="s")
for (N in Ns)
  repl.mut.eq(N=N, m0=mu[1], m1=mu[2]) %>%
  binomialsampling.markov.matrix %>%
  plotstationary(., ylim=0:1)

@

<<asymmetricmutationprobabilities, fig.cap=paste("Completion probabilities over time with $\\mu_0=", mu[1], ", \\mu_1=", mu[2], "$ in a population of $N=", N, "$.", sep=""), fig.subcap=c("Probability of transitions from a population of just variant~0 to just variant~1.", "Probability of transitions from a population of just variant~1 to just variant~0.")>>=
# one direction or the other!
data <- repl.mut.eq(N=N, m0=mu[1], m1=mu[2]) %>%
  binomialsampling.markov.matrix %>%
  plotcompletionprobabilities(., generations=2000)

repl.mut.eq(N=N, m0=mu[2], m1=mu[1]) %>%
  binomialsampling.markov.matrix %>%
  plotcompletionprobabilities(., generations=2000, ylim=c(0, max(data)))
@

\subsubsection{The dynamics of selection}\index{selection,replicator selection}

The presence of a selection bias for individual variants, equivalent to a \emph{replicator selection}, leads to a systematic increase of the selected variant as soon as it is introduced. The selection bias interacts with the mutation probability of the variants, as can be seen in Figure~\ref{fig:selectiontrajectories}.

<<selectiontrajectories, fig.cap=paste("State distribution of all trajectories where the diffusion of the preferentially selected incoming variant is not interrupted."), fig.subcap=paste("Population size $N=", Ns, "$.", sep="")>>=
mu <- mu/10

s <- 0.1
duration <- 500

Ns <- c(50, 100)
for (N in Ns) {
  # mutation probabilities 2/1, 1/1, 1/2
  tightmargin(mfrow=c(1, 3), pty="s")
  repl.mut.eq(N=N, m0=mu[2], m1=mu[1], b=s) %>%
    binomialsampling.markov.matrix %>% makestickytop %>% newchain %>%
    noconditioning(., duration=duration) %>% plotposterior
  repl.mut.eq(N=N, m0=mu[2], m1=mu[2], b=s) %>%
    binomialsampling.markov.matrix %>% makestickytop %>% newchain %>%
    noconditioning(., duration=duration) %>% plotposterior
  repl.mut.eq(N=N, m0=mu[1], m1=mu[2], b=s) %>%
    binomialsampling.markov.matrix %>% makestickytop %>% newchain %>%
    noconditioning(., duration=duration) %>% plotposterior
}
@

When the innovation probabilities are equal~(Figure~\ref{fig:selectiontrajectories2}), we recover the logistic growth dynamics that we also found for the Utterance Selection Model in Section~\ref{sec:usm}.

While the expected trajectories in these three scenarios might differ, their respective stationary distributions are very similar, as can be seen in Figure~\ref{fig:selectionstationaries}. What this tells us is that, if we assume that there is a constant selection pressure in favour of one of the variants applying, then we should (almost) always only ever find this variant attested in language data that we collect synchronically.

% TODO logistic growth graphs:
% 1. make sticky top and see what most likely duration of transition is?
% 2. plot most likely transition that terminates exactly after that number of transitions -> should be logistic (hopefully!??)

<<selectionstationaries, fig.cap=paste("Stationary distribution of the Wright-Fisher model with symmetric mutation and selection, $s=", s, "$ for two different population sizes.", sep="")>>=
tightmargin(mfrow=c(2, 2), pty="s")
for (N in Ns) {
  for (i in c(-0.5, 0.5))
    repl.mut.eq(N=N, m0=mu[1.5+i], m1=mu[1.5-i], b=s) %>%
    binomialsampling.markov.matrix %>%
    plotstationary(., ylim=0:1)
}
@

Figure~\ref{fig:selectioncompletionprobabilities}

<<selectioncompletionprobabilities, fig.cap="Show asymmetric completion probabilities of changes of the two variants given different initial conditions.">>=
# TODO plot different selection and mutation pressures and pick mutation level based on that (selection shouldn't be overpowered)
# TODO set multiple absorbingstates at the top, e.g. more than 90% one variant or something
#tightmargin(mfrow=c(1, 3), pty="s")
#for (b in bs)
#  plotcompletionprobabilitiesperstart(updown.momentum.matrix(N, m0=mu, m1=mu, b=b), maxmomentum=1, main=paste("b=", b, sep=""))
@

%\subsection[The bottleneck in Iterated Learning]{Biases of variation and selection and the bottleneck in Iterated Learning}

<<selectioncompletionprobabilities, fig.cap="Relative influence of mutation and selection pressures given different population sizes.">>=
tightmargin(mfrow=c(1, 3), pty="s")
plot(0, 0)
@

As can be seen in Figure~\ref{fig:TODO}, the \emph{size} of a population plays a crucial rule in how effective different kinds of pressures are. In the same way that completely neutral \emph{diffusion} of a variant is much more likely in smaller populations, the impact of one and the same mutation rate is also much stronger the smaller the population is, whereas the power of selection in such cases declines. This characterisation casts a new light on the workings of the Iterated Learning Model~(ILM) discussed in Section~\ref{sec:ilm}. One of the core ideas of the ILM is that, by imposing a \emph{bottleneck} on the information that is transmitted between generations of learners, biases become exacerbated and express themselves more quickly in the data produced by such Iterated Learning chains.

The present analysis of the relative force of mutation and selection pressures sheds this conclusion in a new light. The imposition of a bottleneck corresponds to a reduction of the population size. Consequently, the biases that get amplified during Iterated Learning experiments are most likely of the \emph{mutation}, rather than the \emph{selection} type. % They are conventions that are more likely to be invented.

% TODO intermediate summary saying that mutation and selection alone are too strong?

\section{A Markov chain state space for momentum}

%The simple models above recapitulate the criticisms presented in Section~\ref{sec:review}: accumulation of error and selection predict~(deterministic) selection only in one way, with no chance of returning.
%Interaction between them does what?

So far, we have used the Wright-Fisher model to explore the quantitative dynamics of different types of pressures that are often invoked in the literature as well as their interaction. The logical next step is to investigate how these results compare to a trend-amplifying bias such as the one implemented by momentum-based selection. On the face of it, the idea of momentum and the assumptions of a Markov model discussed in Section~\ref{sec:markovmodel} seem at odds: to reiterate, the Markov assumption states explicitly that the probability of transitioning into a particular state must only be influenced by a system's current state, not by any previous states or state trajectories.

While the Markov model itself is oblivious to the structure of its state space, a meaningful state space can be constructed. In order to represent a population of $N$ individuals (or memory size of $N$ tokens), \citeauthor{Reali2009} constructed a space of $N+1$ states. On some level, this state space was unstructured: all states had non-zero (if very small) transition probabilities to each other, forming one fully connected state space. But on top of this homogeneous structure there was semantically meaningful organisation of states: each state corresponded to a certain memory state of an agent, with corresponding transition probabilities into the next states~(see Figure~\ref{fig:markovstatespacesimple}).

In order to augment this model with a momentum bias, we simply have to duplicate the number of states: for every state of the \citeauthor{Reali2009} model which represents a certain prevalence $x$ of variant~$0$, we create two additional states, representing the same value of $x$, but with positive and negative momentum terms~$m$.

A schematic visualisation of the shape of this state space is shown in Figure~\ref{fig:markovstatespacemomentum}.

\begin{figure}

\newcommand{\N}{5}
\newcommand{\Nm}{4}
\newcommand{\Nmm}{3}

\begin{subfigure}[b]{\textwidth}
\centering
\begin{tikzpicture}[scale=1.7]
\tikzset{every state/.style={minimum size=1em}}
\node at (-1,-1) {x =};
\node[text=white] at (-2,-1) {m = -1}; % for identical graph width with below

\foreach \x in {0,...,\N} {
  \node at (\x,-1) {$\x$};
  \node[state] (\x) at (\x,0) {};
  \path[->] (\x) edge  [loop left] node {} ();
}

\foreach \x in {0,...,\Nm} {
  \foreach \y in {\N,...,\number\numexpr\x+1}
    \pgfmathtruncatemacro{\diff}{100*(\y-\x)/\N}
    \path[->,draw=white!\diff!black]
      (\x) edge [bend left] node {} (\y)
      (\y) edge [bend left] node {} (\x);
}
\end{tikzpicture}
\caption{State space of the~\citet{Reali2009} Markov model with $N=\N$.}\label{fig:markovstatespacesimple}
\end{subfigure}

\begin{subfigure}[b]{\textwidth}
\centering
\begin{tikzpicture}[scale=1.7]
\tikzset{every state/.style={fill=white,minimum size=1em}}

\node at (-1,-2) {x =};
\node at (-2,-1) {m = -1};
\node at (-2,0) {m = 0};
\node at (-2,1) {m = 1};

% momentum states
\foreach \x in {0,...,\Nm} {
  \pgfmathtruncatemacro{\next}{\x + 1}
  \node[state] (\x-1) at (\x,-1) {};
  \node[state] (\next+1) at (\next,1) {};
  \pgfmathtruncatemacro{\diff}{100/\N}
  \path[<->,draw=white!\diff!black] % immediate reversal
    (\x-1) edge node {} (\next+1); % [bend left=15] to avoid crossing loops
}

% paths along momentum states
\foreach \x in {\Nm,...,1} {
  \pgfmathtruncatemacro{\prev}{\x-1}
  \pgfmathtruncatemacro{\next}{\x+1}

%  \path[->] (\x-1) edge node {} (\prev-1)
%            (\x+1) edge node {} (\next+1);

% along top to right
%  \ifnum \prev>0, draw from \number\numexpr\prev-1\relax only
    \foreach \y in {\prev,...,0} {
      \pgfmathtruncatemacro{\diff}{100*(\x-\y-1)/\N}
      \path[->,draw=white!\diff!black]
        (\x-1) edge [bend left] node {} (\y-1);
    }
%  \fi
% along bottom to left
  \foreach \y in {\N,...,\next} {
    \pgfmathtruncatemacro{\diff}{100*(\y-\x-1)/\N}
    \path[->,draw=white!\diff!black]
      (\x+1) edge [bend left] node {} (\y+1);
  }
}

% long distance reversals
\foreach \x in {0,...,\Nmm} {
  \pgfmathtruncatemacro{\nnext}{\x + 2}
  \foreach \y in {\nnext,...,\N} {
    \pgfmathtruncatemacro{\diff}{100*(\y-\x)/(\N+1)}
    \path[<->,draw=white!\diff!black] (\x-1) edge node {} (\y+1);
    % [bend left=15]
  }
}

% momentum-free states
\foreach \x in {0,...,\N} {
  \node at (\x,-2) {$\x$};
  \node[state] (\x0) at (\x,0) {};
}

% straight paths out of and into momentum-free states
\foreach \x in {0,...,\Nm} {
  \pgfmathtruncatemacro{\next}{\x+1}
  % initiate increase (longest jumps first)
  \foreach \y in {\N,...,\next} {
    \pgfmathtruncatemacro{\diff}{100*(\y-\x-1)/\N}
    \path[->,draw=white!\diff!black] (\x0) edge node {} (\y+1);
  }
  % initiate decrease
  \foreach \y in {0,...,\x} {
    \pgfmathtruncatemacro{\diff}{100*(\x-\y)/\N}
    \path[->,draw=white!\diff!black] (\next0) edge node {} (\y-1);
  }
  % stalling
  \pgfmathtruncatemacro{\diff}{200/\N}
  \path[->,draw=white!\diff!black]
    (\x-1) edge node {} (\x0)
    (\next+1) edge node {} (\next0);
}
% old: curved initation straight to goal
%\pgfmathtruncatemacro{\diff}{100*(\N-1)/\N}
%\path[->,draw=white!\diff!black]
%  (00) edge [bend left=53] node {} (\N+1)
%  (\N0) edge [bend left=53] node {} (0-1);


% completion (drawn in gray right above by default)
%\path[->]
%  (0-1) edge node {} (00)
%  (\N+1) edge node {} (\N0);

% loops
\path[->,out=205,in=155] \foreach \x in {0,...,\N}
  {(\x0) edge [loop] node {} ()};


\end{tikzpicture}

\caption{State space of the Markov model with $N=\N$ and a momentum bias $b>0$.}\label{fig:markovstatespacemomentum}
\end{subfigure}

\caption[Schematic visualisation of the Markov model state space]{Schematic visualisation of the Markov model state space. Colouring of the edges indicates relative probability of the transitions, with darker edges representing more likely transitions.} \label{fig:markovstatespace}
\end{figure}

While the number of states of this momentum model is almost threefold in comparison to the baseline model, the pattern of transitions is actually not much more complex. In particular, the model is not fully connected: every state has exactly $N$ outward transitions, exactly one each to every level of $x=0\ldots N$. 
The semantics of these three parallel states determines which of them a given previous state will transition into: all transitions from states with a lower $x$ go into the $m=1$ state, transitions from higher values of $x$ go into the $m=-1$ state, and only transitions from identical $x$ values enter the state with~$m=0$.

In order to affect the dynamics of the system, probabilities of transitioning between different levels of $x$ are affected by the value of the momentum term: for the middle row with $m=0$, the probabilities of producing a given~$x$ are equivalent to the \citeauthor{Reali2009} model, which means they correspond to the Wright-Fisher model with mutation only, as in Equation~\ref{eq:mutation}.

For the upper and lower rows in the diagram, corresponding to a positive~(top)
 or negative~(bottom) momentum term, the momentum affects the probabilities of producing a certain number of $x$~tokens by exerting a selection pressure on the variant that is currently `trending', i.e. whose frequency $x$ has increased at the last timestep. To this end the transition probabilities out of a state with $m=1$ are calculated according to the Wright-Fisher model with mutation and selection as given in Equation~\ref{eq:mutationselection}, with the selection coefficient $s$ in favour of variant $0$ set to a fixed constant. Conversely, the transition probabilities for states with $m=-1$ are controlled by the same equation, only that the same selection coefficient is selecting variant $1$. Note how, as long as the mutation probabilities $\mu_0, \mu_1$ are equal, this system is again \emph{symmetric}. Even though there are clear paths of directed selection~(e.g. along ) these paths are mirrored on the other side.

A concrete example of a Markov chain transition matrix for such a momentum model with $N=5, \alpha=?, b=$ is given in Table~\ref{tbl:momentumtransitionmatrix}.

<<momentum>>=

momentum.matrix.pos <- function(sourcemomentum, targetmomentum, maxmomentum=1) {
  Nm <- 1+2*maxmomentum
  N <- max(nrow(sourcemomentum), nrow(targetmomentum))
  # column (target) offsets
  col <- matrix(rep(maxmomentum+Nm*0:(N-1), each=N), nrow=N) + targetmomentum
  Nm*N*col + rep(1+maxmomentum+Nm*0:(N-1), N)+sourcemomentum
}

# 'up/down' momentum, i.e. momentum term always one of -1, 0, 1
updown.momentum.matrix <- function(N, ..., b=0) {
  # unbiased (binomial) sampling a la Wright-Fisher
  tm <- repl.mut.matrix(N, ...)[]
  m <- matrix(0, nrow=3*nrow(tm), ncol=3*nrow(tm))

  targetmomentums <- upper.tri(tm) - lower.tri(tm)
  # first take over all uninfluenced production probabilities
  m[momentum.matrix.pos(0, targetmomentums)] <- tm
  # calculate momentum-influenced production probabilities and add them
  for (n in 1:nrow(tm)) {
    posmtargets <- momentum.matrix.pos(1, targetmomentums)
    m[posmtargets] <- binomialsampling.markov.matrix(repl.mut.eq(N, ..., b=b))[]
    negmtargets <- momentum.matrix.pos(-1, targetmomentums)
    m[negmtargets] <- binomialsampling.markov.matrix(repl.mut.eq(N, ..., b=-b))[]
  }
  newchain(m, name=paste("Binary (up/down) momentum with b", b, sep="="), states=as.vector(t(outer(0:N, c("-", "=", "+"), function(x, m)paste(x, m, sep="")))))
}
#updown.momentum.matrix(3, b=1)

alpha <- .1
b <- .3
latextable(updown.momentum.matrix(3, alpha0=alpha, b=b)[], caption="Markov chain transition matrix with momentum.", label="tbl:momentumtransitionmatrix", floating.environment="sidewaystable")
@

How do the dynamics of this momentum selection model differ from the original, mutation-only model? A direct comparison of the two models' stationary distributions for various settings of the regularisation parameter~$\alpha$ is shown in Figure~\ref{fig:momentumstationary}.

First of all, we can test whether the model is actually equivalent by setting the momentum bias to $b=0$~(Figure~\ref{fig:momentumstationary1})
Despite the bias being ineffective in this model, the colour indication of the momentum term is informative: it shows how much of the time the model \\emph{remains} at a given proportion, resulting in a momentum term of $0$ (indicated in white).

<<momentumstationary, fig.cap=paste("Stationary distributions of Markov chains with momentum in a population of $N=", N, "$, various $\\alpha, b$.", sep=""), fig.subcap=c("Without a momentum bias~($b=0$) the stationary distribution is identical to \\citet{Reali2009}'s \\emph{averaging} learner~(compare Figure~\\ref{fig:averagerstationarydistribution}).", paste("With a momentum bias of $b=", bs[2], "$, the model naturally avoids probability matching as the model preferentially sweeps through this middle region in a directed fashion. The time spent in momentum-free states in the middle reduces. TODO Explain red=$1$, blue$=-1$.", sep=""), paste("With an even higher momentum bias $b=", bs[3], "$, the model spends almost no time in the middle region.", sep=""))>>=

N <- 21

plotmomentumstationary <- function(markovchain, maxmomentum=0, symmetric=TRUE, col=temp.colors(-maxmomentum, maxmomentum, intensity=0.5), xlab="x", ...) { # TODO space default?
  if (symmetric) {
    # symmetric colouring: plot data with left half zeroed first
    st <- matrix(steadyStates(markovchain), nrow=1+2*maxmomentum)
    halfway <- ceiling(dim(st)[2]/2)
    barplot(cbind(matrix(0, nrow=3, ncol=halfway), st[dim(st)[1]:1,(halfway+1):(dim(st)[2])]), col=rev(col), names.arg=0:(dim(st)[2]-1), xlab=xlab, ...)
    barplot(st[,1:halfway], col=col, add=TRUE)
  } else { # asymmetric (+-momentum doesn't flip at mid-point)
    plotstationary(st=matrix(steadyStates(markovchain), nrow=1+2*maxmomentum), col=col, ...)
#    plotstationary(st=t(colSums(matrix(steadyStates(markovchain), nrow=1+2*maxmomentum))), ...)
  }
}
#par(mfrow=1:2)
#plotmomentumstationary(updown.momentum.matrix(10, alpha0=1), 1, symmetric=FALSE)
#plotmomentumstationary(updown.momentum.matrix(10, alpha0=1), 1, symmetric=TRUE)

alphas <- c(0.5, 1, 2)
bs <- c(0, 1, 3)

for (b in bs) {
  tightmargin(mfrow=c(1,3), pty="s")
  for (alpha in alphas)
    plotmomentumstationary(updown.momentum.matrix(N, alpha0=alpha, b=b), 1, main=formatalpha(alpha), ylim=c(0, 0.25))
}

# stationary with b=0 is actually identical to the averaging one
#for (alpha in c(0.2, 1, 2)) {
#  plotstationary(bilm.transition.matrix.average(N, alpha))
#  plotmomentumstationary(updown.momentum.matrix(N, alpha0=alpha), 1)
#}
@

Assuming a new variant is introduced, what is the likelihood of this actuated transition to actually go to completion given different strengths of the momentum bias? This is shown in Figure~\ref{fig:symmetricmomentumcompletionprobabilities}.

<<symmetricmomentumcompletionprobabilities, fig.cap="Probability of an actuated transition completing as a function of the current frequency of the incoming variant, assuming the variant currently has positive~(upwards) momentum. Symmetric mutation probabilities.">>=

# merge the absorbingstates of the transition matrix together into one absorbing state that only transitions to itself
mergeabsorbingstates <- function(m, absorbingstates) {
  # take subset of transition matrix, only leave one state for all states
  ma <- m[-absorbingstates[-1], -absorbingstates[-1]]
  # merge (sum) the final absorbingrows columns together for every row
  ma[,absorbingstates[1]] <- rowSums(m[-absorbingstates[-1], absorbingstates])
  # make new merged absorbing state only transition to itself
  ma[absorbingstates[1],] <- 0
  ma[absorbingstates[1], absorbingstates[1]] <- 1
  return(ma)
}

plotcompletionprobabilitiesperstart <- function(transitionmatrix, maxmomentum=0, add=FALSE, ...) {
  momentumstates <- 1 + 2*maxmomentum
  freqs <- 0:(dim(transitionmatrix)/momentumstates-1)
  # squash top/bottom states together into two absorbing states
  m <- mergeabsorbingstates(transitionmatrix[], 1:momentumstates)
  transitionmatrix <- newchain(mergeabsorbingstates(m, (nrow(m)-momentumstates+1):nrow(m)))
  # indices of states which have positive momentum
  startindices <- c(1, 1+freqs[-c(1, length(freqs))]*3, dim(transitionmatrix))
  ps <- sapply(startindices, function(i) successprobability(transitionmatrix, i))
  if (add)
    points(freqs, ps, ...)
  else
    plot(freqs, ps, xlab="initial frequency", ylab="probability of diffusion without interruption", ylim=0:1, ...)
}

alpha <- .01
mu <- N.alpha.to.u(N, alpha)

tightmargin(mfrow=c(1, 3), pty="s")
for (b in bs)
  plotcompletionprobabilitiesperstart(updown.momentum.matrix(N, m0=mu, m1=mu, b=b), maxmomentum=1, main=paste("b=", b, sep=""))
@

What about the case of \emph{asymmetric} mutation probabilities in combination with the \emph{symmetric} momentum bias? See Figure~\ref{fig:asymmetricmomentumcompletionprobabilities}.

<<asymmetricmomentumcompletionprobabilities, fig.cap=paste("Dynamics of momentum-based selection with asymmetric mutation probabilities for different strengths of the momentum bias~($b=", paste(bs, collapse=", "), "$, from left to right).", sep=""), fig.subcap=c("Stationary distribution", "Completion probabilities for transitions where the incoming variant is the one that is \\emph{more} likely to be introduced through mutation~(circles) as well as for transitions where the incoming variant is the one that is \\emph{less} likely to be introduced through mutation~(crosses).")>>=
bs <- c(0.5, 1, 2)

m1 <- N.alpha.to.u(N, alpha)
m0 <- m1/5

tightmargin(mfrow=c(1, length(bs)), pty="s")
for (b in bs)
  plotmomentumstationary(updown.momentum.matrix(N, m0=m0, m1=m1, b=b), 1)

tightmargin(mfrow=c(1, length(bs)), pty="s")
for (b in bs) {
  plotcompletionprobabilitiesperstart(updown.momentum.matrix(N, m0=m0, m1=m1, b=b), maxmomentum=1)
  plotcompletionprobabilitiesperstart(updown.momentum.matrix(N, m0=m1, m1=m0, b=b), maxmomentum=1, pch=4, add=TRUE)
}
#tightmargin(mfrow=c(1, 3), pty="s")
#for (b in bs)
#  plotcompletionprobabilitiesperstart(updown.momentum.matrix(N, alpha0=alpha*5, alpha1=alpha, b=b), maxmomentum=1, main=paste("b=", b, sep=""))
@



<<innovationprobabilities, fig.cap=paste("The role of asymmetric mutation probabilities on the synchronic distribution of variants. Only the probability of innovating variant~1 varies between plots, all other parameters are held constant~($\\mu_0=", m0, ", b=1$).", sep="")>>=
b <- 1
m1s <- m0*c(2, 5, 10)
tightmargin(mfrow=c(1, length(alphas)), pty="s")
for (m1 in m1s)
  plotmomentumstationary(updown.momentum.matrix(N, m0=m0, m1=m1, b=b), 1, main=bquote(mu[1] ~ "=" ~ .(m1)))
@

